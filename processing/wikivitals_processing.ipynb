{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIKIVITALS Processing\n",
    "\n",
    "The aim of this notebook is to get all the articles from Wikivitals level 5 using :\n",
    "- the lists defined in this page () and all the redirections\n",
    "- a wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "# Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from bz2 import BZ2File\n",
    "from lxml import etree as et\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unidecode\n",
    "import wikitextparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22819068\n"
     ]
    }
   ],
   "source": [
    "extract_from_dump = False\n",
    "\n",
    "use_vitals_from_web = False\n",
    "\n",
    "wikivitals_root_page = \"Wikipedia:Vital articles/Level/5 - Wikipedia\"\n",
    "wikivitals_root_page_id = 55702953\n",
    "\n",
    "wikidump_splits_path = \"C:/Users/Antoine/Downloads/enwiki-split\"\n",
    "\n",
    "# I used the wikipedia library to find it (the only online call)\n",
    "page = wikipedia.page(wikivitals_root_page)\n",
    "print(page.pageid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "# Namespaces defined here: https://en.wikipedia.org/wiki/Wikipedia:Namespace\n",
    "# 'Image:' added \n",
    "\n",
    "namespaces = ['Talk:','User:','User talk:', 'Wikipedia:','Wikipedia talk:',\n",
    "    'File:','File talk:','MediaWiki:','MediaWiki talk:','Template:','Template talk:',\n",
    "    'Help:','Help talk:','Category:','Category talk:','Portal:','Portal talk:','Draft:',\n",
    "    'Draft talk:','TimedText:','TimedText talk:','Module:','Module talk:', 'Image:']\n",
    "namespaces = namespaces + [i.lower() for i in namespaces]\n",
    "namespaces = namespaces + [':'+ i for i in namespaces]\n",
    "# wikiprojects = ['wikt:', 'Wiktionary:', 's:', 'wikisource:', 'w:', 'iarchive:', 'b:']\n",
    "# wikiprojects = wikiprojects + [i.lower() for i in wikiprojects] \n",
    "# others = ['doi:']\n",
    "\n",
    "\n",
    "def wikilinks_namespace0(wikitext, string_format = True):\n",
    "    \"\"\"\n",
    "    Extract links with namespace 0 in wikitext \n",
    "\n",
    "    Input:\n",
    "    * wikitext: str \n",
    "\n",
    "    Output:\n",
    "    * links: list of str (list of canonical name of pages)\n",
    "    \"\"\"\n",
    "    # Update the prefix used to exclude links that are in another namespaces or links to another wikiprojects\n",
    "    # tmp = namespaces + wikiprojects + others\n",
    "    # prefixes = tmp +  [':'+i for i in tmp]\n",
    "    prefixes = tuple(namespaces)\n",
    "\n",
    "    links_ = wikitextparser.parse(wikitext).wikilinks\n",
    "    if string_format:\n",
    "        links = [s.title.strip() for s in links_ if not s.title.strip().startswith(prefixes)]\n",
    "        excluded = [s.title.strip() for s in links_ if s.title.strip().startswith(prefixes)]\n",
    "    else:\n",
    "        links = [s for s in links_ if not s.title.strip().startswith(prefixes)]\n",
    "        excluded = [s for s in links_ if s.title.strip().startswith(prefixes)]\n",
    "    return(links, excluded)\n",
    "\n",
    "# def wikilinks_images(wikitext, string_format = True):\n",
    "#     \"\"\"\n",
    "#     Extract links with namespace 0 in wikitext \n",
    "\n",
    "#     Input:\n",
    "#     * wikitext: str \n",
    "\n",
    "#     Output:\n",
    "#     * links: list of str (list of canonical name of pages)\n",
    "#     \"\"\"\n",
    "#     links_ = wikitextparser.parse(wikitext).wikilinks\n",
    "#     if string_format:\n",
    "#         links = [s.title for s in links_ if s.title.startswith('Image:')]\n",
    "#     else:\n",
    "#         links = [s for s in links_ if s.title.startswith('Image:')]\n",
    "#     return(links)\n",
    "\n",
    "def normalize_labels(wikitext):\n",
    "    cleaned_string = wikitextparser.remove_markup(wikitext.title)\n",
    "    return(cleaned_string.split(' (')[0]) # WHY ?\n",
    "\n",
    "def get_highest_level_headers(wikitext, highest_level_authorized = 1):\n",
    "    sections_ = wikitextparser.parse(wikitext).sections\n",
    "\n",
    "    # Get all headers (all those with level at least equal to h1)\n",
    "    header_level = [(h, (len(str(h)) - len(str(h).lstrip('=')))) for h in sections_ if not str(h) == '']\n",
    "    header_level = [(h, length) for h,length in header_level if length >= highest_level_authorized]\n",
    "    # Get the \"highest level\" among the headers (h1 > h2 > h3 > h4 > h5 > h6)\n",
    "    try:\n",
    "        highest_level = min([i for j,i in header_level])\n",
    "    except:\n",
    "        highest_level = 0 #if list empty\n",
    "    # Keep only the highest level headers \n",
    "    # Note that h is wikitext of a Section = its content \n",
    "    filtered_headers = [h for h, j in header_level if j==highest_level]\n",
    "    return filtered_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Vital articles\n",
    "\n",
    "## Vital articles\n",
    "\n",
    "From Wikipedia:\n",
    "\"Vital articles are lists of subjects for which the English Wikipedia should have corresponding featured-class articles. They serve as centralized watchlists to track the quality status of **Wikipedia's most important articles** and to give editors guidance on which articles to prioritize for improvement.\"\n",
    "\n",
    "The Vital articles of level 5 list **about 50,000 articles** (the target value) that have been **manually categorized** by the wikipedia editors. \n",
    "\n",
    "A root page contains the links to the different Vital Articles that are in the namespace 'Wikipedia' (number 4). \n",
    "- [the namespaces of wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Namespace)\n",
    "- [the root page](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/5) (be careful, here we use a dump frozen at a certain date to extract the information, this page can be different from the one extracted in this work)\n",
    "\n",
    "## Extraction\n",
    "\n",
    "Objectives:\n",
    "- Collect and store the wiki ids of level 5 Vital articles \n",
    "- Extract the wikitext from the Vital level 5 articles and store it\n",
    "\n",
    "Outputs:\n",
    "- In the folder './outputs' the file 'wikivitals_pageid.txt' contains the wiki ids and the canonical names of the Vital articles of level 5  \n",
    "- In the folder './wikivitals-pages-wikitext' are all the wikitext of the Vital articles (32 files + the root page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia:Vital articles/Level/5/People/Writers and journalists\n",
      "Wikipedia:Vital articles/Level/5/People/Artists, musicians, and composers\n",
      "Wikipedia:Vital articles/Level/5/People/Entertainers, directors, producers, and screenwriters\n",
      "Wikipedia:Vital articles/Level/5/People/Philosophers, historians, political and social scientists\n",
      "Wikipedia:Vital articles/Level/5/People/Religious figures\n",
      "Wikipedia:Vital articles/Level/5/People/Politicians and leaders\n",
      "Wikipedia:Vital articles/Level/5/People/Military personnel, revolutionaries, and activists\n",
      "Wikipedia:Vital articles/Level/5/People/Scientists, inventors, and mathematicians\n",
      "Wikipedia:Vital articles/Level/5/People/Sports figures\n",
      "Wikipedia:Vital articles/Level/5/People/Miscellaneous\n",
      "Wikipedia:Vital articles/Level/5/History\n",
      "Wikipedia:Vital articles/Level/5/Geography/Physical\n",
      "Wikipedia:Vital articles/Level/5/Geography/Countries\n",
      "Wikipedia:Vital articles/Level/5/Geography/Cities\n",
      "Wikipedia:Vital articles/Level/5/Arts\n",
      "Wikipedia:Vital articles/Level/5/Philosophy and religion\n",
      "Wikipedia:Vital articles/Level/5/Everyday life\n",
      "Wikipedia:Vital articles/Level/5/Everyday life/Sports, games and recreation\n",
      "Wikipedia:Vital articles/Level/5/Society and social sciences/Social studies\n",
      "Wikipedia:Vital articles/Level/5/Society and social sciences/Politic and economic\n",
      "Wikipedia:Vital articles/Level/5/Society and social sciences/Culture\n",
      "Wikipedia:Vital articles/Level/5/Biological and health sciences/Biology\n",
      "Wikipedia:Vital articles/Level/5/Biological and health sciences/Animals\n",
      "Wikipedia:Vital articles/Level/5/Biological and health sciences/Plants\n",
      "Wikipedia:Vital articles/Level/5/Biological and health sciences/Health\n",
      "Wikipedia:Vital articles/Level/5/Physical sciences/Basics and measurement\n",
      "Wikipedia:Vital articles/Level/5/Physical sciences/Astronomy\n",
      "Wikipedia:Vital articles/Level/5/Physical sciences/Chemistry\n",
      "Wikipedia:Vital articles/Level/5/Physical sciences/Earth science\n",
      "Wikipedia:Vital articles/Level/5/Physical sciences/Physics\n",
      "Wikipedia:Vital articles/Level/5/Technology\n",
      "Wikipedia:Vital articles/Level/5/Mathematics\n",
      "\n",
      "\n",
      "32 Vital articles (level 5) found\n",
      "Wiki IDs have been stored\n"
     ]
    }
   ],
   "source": [
    "import mwxml\n",
    "import mwtypes\n",
    "import wikitextparser\n",
    "# Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "\n",
    "\n",
    "# Main page wikivitals:\n",
    "wikipedia_root_page_wt = open(\"./outputs/wikivitals-pages-wikitext/Vital articles-Level-5.wikitxt\", 'r', encoding='utf8')\n",
    "wikitext = wikipedia_root_page_wt.read()\n",
    "wikipedia_root_page_wt.close()\n",
    "\n",
    "# Extract links in the page from the wikitext\n",
    "_, excluded = wikilinks_namespace0(wikitext) # Vital articles are NOT in namespace 0\n",
    "\n",
    "# All links to Vital articles starts with 'Wikipedia:Vital articles/Level/5/'\n",
    "# Once find, we store in a file the wiki id of each Vital article for later use\n",
    "vital_articles_wikiids = []\n",
    "outfile = open(\"./outputs/wikivitals_pageid.txt\", 'w')\n",
    "for i in excluded:\n",
    "    if i.startswith('Wikipedia:Vital articles/Level/5/'):\n",
    "        print(i)\n",
    "        wikiid = wikipedia.page(i).pageid\n",
    "        vital_articles_wikiids.append(int(wikiid))\n",
    "        outfile.write(f'{wikiid}\\t{i}\\n')\n",
    "        \n",
    "outfile.close()\n",
    "print('\\n')\n",
    "print(f'{len(vital_articles_wikiids)} Vital articles (level 5) found')\n",
    "print('Wiki IDs have been stored') \n",
    "\n",
    "\n",
    "# Each split of the dump has the same format:\n",
    "# enwiki-20220401-pages-articles{split_part}.xml-p{start_id}p{end_id}\n",
    "# where {start_id} and {end_id} # define the interval of wiki id in this \n",
    "# split of the dump.\n",
    "# \n",
    "# In order not to explore the whole dump to extract the wikitext content \n",
    "# of the Vital articles, we will identify from the ids of these pages the parts \n",
    "# of the split to browse\n",
    "dump_files = os.listdir(wikidump_splits_path)\n",
    "intervals = [s[:-4].split('.xml-p')[1].split('p') for s in dump_files]\n",
    "intervals = [(int(i), int(j)) for i,j in intervals]\n",
    "mask = [False]*len(intervals)\n",
    "for wikiid in vital_articles_wikiids:\n",
    "    for k in range(len(intervals)):\n",
    "        if (intervals[k][0] <= wikiid <= intervals[k][1]):\n",
    "            mask[k] = True\n",
    "# files_selected = list of dump parts to explore\n",
    "files_selected = [wikidump_splits_path + '/' + dump_files[k] for k in range(len(dump_files)) if mask[k]] \n",
    "\n",
    "\n",
    "\n",
    "def page_info(dump, path):\n",
    "    \"\"\"\n",
    "    Read the dump page per page\n",
    "    (can be paralellized a priori - can't make it work)\n",
    "\n",
    "    Input: \n",
    "    * dump: list of Wikidumps (with format .xml.bz2)\n",
    "    * path: str - path to Wikidump    \n",
    "    \"\"\"\n",
    "    for page in dump:\n",
    "        tstamp = mwtypes.Timestamp(0)\n",
    "        if page.id in page_ids:\n",
    "            for revision in page:\n",
    "                if revision.timestamp > tstamp:\n",
    "                    last_revision = revision\n",
    "                    tstamp = revision.timestamp\n",
    "                text = last_revision.text\n",
    "            yield page.id, page.title, page.redirect, text\n",
    "\n",
    "\n",
    "# Reads each part of the dump that has been identified as containing the \n",
    "# wikitext of at least one Vital article in order to extract these wikitexts \n",
    "# and save them in the './outputs/wikivitals-pages-wikitext/' folder\n",
    "for file in files_selected:\n",
    "    for id, title, redirect, text in mwxml.map(page_info, [file]):\n",
    "        \n",
    "        title_ = title.replace('/', '-')\n",
    "        filename = \"./outputs/wikivitals-pages-wikitext/\" + title_ + \".wikitxt\"\n",
    "        f = open(filename, \"w\", encoding = 'utf8')\n",
    "        f.write(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the classes (level 0 and 1)\n",
    "\n",
    "All links have the same format Wikipedia:Vital articles/Level/5/' + class0 + '/' + class1\n",
    "\n",
    "Ex: 'Wikipedia:Vital articles/Level/5/People/Entertainers, directors, producers, and screenwriters'\n",
    "We want to extract {'class0': 'People', 'class1':'Entertainers, directors, producers, and screenwriters'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia:Vital articles/Level/5/People/Artists, musicians, and composers:{'class0': 'People', 'class1': 'Artists, musicians, and composers'}\n"
     ]
    }
   ],
   "source": [
    "# Extract the classes (level 0 and 1)\n",
    "\n",
    "start_str = 'Wikipedia:Vital articles/Level/5/'\n",
    "\n",
    "lclasses = dict()\n",
    "for link in links_str:\n",
    "    classes = link.strip()[len(start_str):].split('/')\n",
    "    if len(classes) == 1:\n",
    "        classes.append(classes[0])\n",
    "    lclasses[link] = {\n",
    "        'class0': classes[0], \n",
    "        'class1': classes[1]\n",
    "        }\n",
    "\n",
    "# Display an example\n",
    "print(f'{list(lclasses.keys())[0]}:{lclasses[list(lclasses.keys())[0]]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Wikivitals pages from the dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mwxml\n",
    "# import mwtypes\n",
    "# import wikitextparser\n",
    "# # Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# # # Internal path to the Wikipedia dump \n",
    "# # path  = \"C:/Users/Antoine/Downloads/\" + \"enwiki-20220401-pages-articles.xml.bz2\"\n",
    "# # # path =  \"C:/Users/Antoine/Downloads/\" + \"enwiki-20220401-pages-articles24.xml-p55064554p56564553.bz2\"\n",
    "\n",
    "\n",
    "# page_ids = list(pageid_wikivitals.keys())\n",
    "\n",
    "\n",
    "# # # Matches all h1 titles (h-infinite in reality but no matter)\n",
    "# # # Title h4 in Wikitext: ==== a h4 title ====\n",
    "# # h_2_5_title = re.compile(r'\\n(===*([^=].+?)=*[==])') # group 1: the header with marks, group 2 the header content\n",
    "\n",
    "\n",
    "# # # LINKS\n",
    "\n",
    "# # # Matches links with no nested link\n",
    "# # # [[a link]] => IT'S A MATCH!\n",
    "# # # [[This is [[a link]] in a link]] => SORRY, NO MATCH!\n",
    "# # # Basic structure of a link in wikitext : [[canonical name | name used in context]]\n",
    "# # internal_link_1_level = re.compile(r\"\\[\\[(([^\\[\\]|#]*)[#]*[^\\[\\]|]*[|]*?[^\\[\\]|]*?)\\]\\]\") # group 1: the content of the link, group 2: the canonical name\n",
    "# # # Matches 'File' links (without any nested link)\n",
    "# # file_link_1_level = re.compile(r\"\\[\\[File:[^\\[]*?\\]\\]\") \n",
    "# # # Matches 'Category' links (without any nested link)\n",
    "# # category_link_1_level = re.compile(r\"\\[\\[Category:([^\\[]*?)\\]\\]\") # group 1: the category name\n",
    "# # # Matches 'Wikipedia' links (without any nested link) \n",
    "# # # Wikipedia links are NOT links to other articles we want to keep\n",
    "# # wikipedia_link_1_level = re.compile(r\"\\[\\[Wikipedia:[^\\[]*?\\]\\]\")\n",
    "# # # Matches 'Image' links (without any nested link)\n",
    "# # image_link_1_level = re.compile(r\"\\[\\[Image:[^\\[]*?\\]\\]\")\n",
    "\n",
    "\n",
    "# # def find_internal_links(T):\n",
    "# #     \"\"\"\n",
    "# #     Extract and replace / remove links\n",
    "# #     Image, Wikipedia (=/= links to other wikipedia pages), and File links are removed\n",
    "# #     Category links are replaced by their value\n",
    "# #     Links to other wikipedia articles are replaced by the canonical name of the page\n",
    "\n",
    "# #     Input: \n",
    "# #     * T: a text in wikitext\n",
    "\n",
    "# #     Output:\n",
    "# #     * (L, T_): \n",
    "# #         L: List of canonical names of pages\n",
    "# #         T_: Text with links removed or replaced    \n",
    "# #     \"\"\"\n",
    "# #     tmp = T\n",
    "# #     links = []\n",
    "# #     # We allow a depth of search for nesting of 10\n",
    "# #     for i in range(5):\n",
    "# #         # replace category by the category name\n",
    "# #         tmp = category_link_1_level.sub(r'\\1', tmp)\n",
    "# #         # remove files\n",
    "# #         tmp = file_link_1_level.sub(r' ', tmp)\n",
    "# #         # remove wikipedia links\n",
    "# #         tmp = wikipedia_link_1_level.sub(r' ', tmp)\n",
    "# #         # remove image links\n",
    "# #         tmp = image_link_1_level.sub(r' ', tmp)\n",
    "# #         # Find all remaining links\n",
    "# #         links += internal_link_1_level.findall(tmp)\n",
    "# #         # replace links without nested links in the text by their content\n",
    "# #         tmp = internal_link_1_level.sub((r'\\2'), tmp)\n",
    "# #     return [l for l, _ in links], tmp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# files = [wikidump_splits_path + '/' + name for name in list(file_2_pages.keys()) ]\n",
    "\n",
    "# def page_info(dump, path):\n",
    "#     \"\"\"\n",
    "#     Read the dump page per page\n",
    "#     (can be paralellized a priori - can't make it work)\n",
    "\n",
    "#     Input: \n",
    "#     * dump: list of Wikidumps (with format .xml.bz2)\n",
    "#     * path: (unused) str - path to Wikidump    \n",
    "#     \"\"\"\n",
    "#     cnt = 0\n",
    "#     for page in dump:\n",
    "#         cnt +=1\n",
    "#         if cnt%10000 == 0:\n",
    "#             print(cnt)\n",
    "#         tstamp = mwtypes.Timestamp(0)\n",
    "#         if page.id in page_ids:\n",
    "#             for revision in page:\n",
    "#                 if revision.timestamp > tstamp:\n",
    "#                     last_revision = revision\n",
    "#                     tstamp = revision.timestamp\n",
    "#                 text = last_revision.text\n",
    "#             yield page.id, page.title, page.redirect, text\n",
    "\n",
    "\n",
    "# for file in files:\n",
    "#     for id, title, redirect, text in mwxml.map(page_info, [file]):\n",
    "        \n",
    "#         title_ = title.replace('/', '-')\n",
    "#         filename = \"./outputs/wikivitals-pages-wikitext/\" + title_ + \".wikitxt\"\n",
    "#         f = open(filename, \"w\", encoding = 'utf8')\n",
    "#         f.write(text)\n",
    "#         # print(text, file=f)\n",
    "#         f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of the vital articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of articles in hierarchy: 48649\n",
      "Number of articles recovered: 48596\n",
      "Number of duplicates in the hierarchy: 53\n"
     ]
    }
   ],
   "source": [
    "import wikitextparser\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "count_headers = 0\n",
    "\n",
    "\n",
    "# Will be used to store the class hierarchy\n",
    "labels_hierarchy = {'num_articles_expected': 0, 'num_articles': 0}\n",
    "# Will be used to store the pages canonical names and their labels\n",
    "articles_classification = dict()\n",
    "\n",
    "\n",
    "dir = './outputs/wikivitals-pages-wikitext/'\n",
    "files = os.listdir(dir)\n",
    "for file in files: # Read each Vital article one by one\n",
    "    f = open(dir + file, 'r', encoding='utf8')\n",
    "    wikitext = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Get the \"highest level\" among the headers (h1 > h2 > h3 > h4 > h5 > h6)\n",
    "    filtered_headers = get_highest_level_headers(wikitext)\n",
    "\n",
    "    count_headers += len(filtered_headers)\n",
    "    \n",
    "\n",
    "    # Extract class 0 and 1 from file name\n",
    "    # class 2 elements are highest level header titles\n",
    "    # and add them to class hierarchy\n",
    "    filename_split = file[:-8].split('-')\n",
    "    class0 = filename_split[3]\n",
    "    if len(filename_split) == 5:\n",
    "        class1 = filename_split[4]\n",
    "    else:\n",
    "        class1 = class0\n",
    "    classes2 = [normalize_labels(s) for s in filtered_headers]\n",
    "    if not class0 in labels_hierarchy.keys():\n",
    "        labels_hierarchy[class0] = {class1: {'num_articles_expected': 0, 'num_articles': 0}, 'num_articles_expected': 0, 'num_articles': 0}\n",
    "    else:\n",
    "        labels_hierarchy[class0][class1] = {'num_articles_expected': 0, 'num_articles': 0}\n",
    "    for class2 in classes2:\n",
    "        labels_hierarchy[class0][class1][class2] = {'num_articles_expected': 0, 'num_articles': 0}\n",
    "    \n",
    "\n",
    "    headers_n_links = [ (normalize_labels(s), wikilinks_namespace0(str(s))[0]) for s in filtered_headers]\n",
    "    for class2, links in headers_n_links:\n",
    "        # Update the expected count of articles in the label hierarchy\n",
    "        labels_hierarchy[class0][class1][class2]['num_articles_expected'] = len(links)\n",
    "        labels_hierarchy[class0][class1]['num_articles_expected'] += len(links)\n",
    "        labels_hierarchy[class0]['num_articles_expected'] += len(links)\n",
    "        labels_hierarchy['num_articles_expected'] += len(links)\n",
    "\n",
    "        for link in links:\n",
    "            articles_classification[link] = {\n",
    "                'class0': class0,\n",
    "                'class1': class1,\n",
    "                'class2': class2\n",
    "            }\n",
    "\n",
    "print('\\n')\n",
    "print(f'Number of articles in hierarchy: {labels_hierarchy[\"num_articles_expected\"]}')\n",
    "print(f'Number of articles recovered: {len(list(articles_classification.keys()))}')\n",
    "print(f'Number of duplicates in the hierarchy: {labels_hierarchy[\"num_articles_expected\"] - len(list(articles_classification.keys()))}')  \n",
    "\n",
    "\n",
    "# store classes hierarchy\n",
    "outfile_classification = \"./outputs/classification_unfiltered.txt\"\n",
    "f = open(outfile_classification, 'w', encoding='utf8')\n",
    "for k, v in articles_classification.items():\n",
    "    f.write(f'{k}\\t{v[\"class0\"]}\\t{v[\"class1\"]}\\t{v[\"class2\"]}\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the list of articles found\n",
    "\n",
    "We'll use a dump to do this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the full wikitext of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles1.xml-p1p41242.bz2\n",
      "1000 articles found\n",
      "2000 articles found\n",
      "3000 articles found\n",
      "4000 articles found\n",
      "5000 articles found\n",
      "6000 articles found\n",
      "7000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles10.xml-p4045403p5399366.bz2\n",
      "8000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p5399367p6899366.bz2\n",
      "9000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p6899367p7054859.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p7054860p8554859.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p8554860p9172788.bz2\n",
      "10000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p10672789p11659682.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p9172789p10672788.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p11659683p13159682.bz2\n",
      "11000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p13159683p14324602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p14324603p15824602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p15824603p17324602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p17324603p17460152.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p17460153p18960152.bz2\n",
      "12000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p18960153p20460152.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p20460153p20570392.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p20570393p22070392.bz2\n",
      "13000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p22070393p23570392.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p23570393p23716197.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p23716198p25216197.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p25216198p26716197.bz2\n",
      "14000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p26716198p27121850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p27121851p28621850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p28621851p30121850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p30121851p31308442.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles2.xml-p41243p151573.bz2\n",
      "15000 articles found\n",
      "16000 articles found\n",
      "17000 articles found\n",
      "18000 articles found\n",
      "19000 articles found\n",
      "20000 articles found\n",
      "21000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p31308443p32808442.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p32808443p34308442.bz2\n",
      "22000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p34308443p35522432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p35522433p37022432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p37022433p38522432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p38522433p39996245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p39996246p41496245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p41496246p42996245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p42996246p44496245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p44496246p44788941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p44788942p46288941.bz2\n",
      "23000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p46288942p47788941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p47788942p49288941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p49288942p50564553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p50564554p52064553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p52064554p53564553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p53564554p55064553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p55064554p56564553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p56564554p57025655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p57025656p58525655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p58525656p60025655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p60025656p61525655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p61525656p62585850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles26.xml-p62585851p63975909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p63975910p65475909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p65475910p66975909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p66975910p68475909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p68475910p69975909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p69975910p70448383.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles3.xml-p151574p311329.bz2\n",
      "24000 articles found\n",
      "25000 articles found\n",
      "26000 articles found\n",
      "27000 articles found\n",
      "28000 articles found\n",
      "29000 articles found\n",
      "30000 articles found\n",
      "31000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles4.xml-p311330p558391.bz2\n",
      "32000 articles found\n",
      "33000 articles found\n",
      "34000 articles found\n",
      "35000 articles found\n",
      "36000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles5.xml-p558392p958045.bz2\n",
      "37000 articles found\n",
      "38000 articles found\n",
      "39000 articles found\n",
      "40000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles6.xml-p958046p1483661.bz2\n",
      "41000 articles found\n",
      "42000 articles found\n",
      "43000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles7.xml-p1483662p2134111.bz2\n",
      "44000 articles found\n",
      "45000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles8.xml-p2134112p2936260.bz2\n",
      "46000 articles found\n",
      "47000 articles found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles9.xml-p2936261p4045402.bz2\n",
      "48000 articles found\n",
      "48512\n"
     ]
    }
   ],
   "source": [
    "import mwxml\n",
    "import mwtypes\n",
    "import wikitextparser\n",
    "# Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "\n",
    "import re\n",
    "\n",
    "# Internal path to the Wikipedia dump split\n",
    "wikidump_splits_path = \"C:/Users/Antoine/Downloads/enwiki-split\"\n",
    "dump_files = os.listdir(wikidump_splits_path)\n",
    "files = [wikidump_splits_path + '/' + f for f in dump_files]\n",
    "\n",
    "\n",
    "# Set of all article canonical titles\n",
    "article_titles = set(list(articles_classification.keys()))\n",
    "\n",
    "\n",
    "\n",
    "def page_info(dump, path):\n",
    "    \"\"\"\n",
    "    Read the dump page per page. Yield infos (id, title, redirect, and text)\n",
    "    only for pages in the set of wikivitals articles (global var)\n",
    "    (can be paralellized, can't make it work)\n",
    "\n",
    "    Input: \n",
    "    * dump: list of Wikidumps (with format .xml.bz2)\n",
    "    * path: str - path to Wikidump split   \n",
    "    \"\"\"\n",
    "    for page in dump:\n",
    "        tstamp = mwtypes.Timestamp(0)\n",
    "        if page.title in article_titles and page.redirect == None and page.namespace == 0:\n",
    "            for revision in page:\n",
    "                if revision.timestamp > tstamp:\n",
    "                    last_revision = revision\n",
    "                    tstamp = revision.timestamp\n",
    "                text = last_revision.text\n",
    "            yield page.id, page.title, page.redirect, text\n",
    "\n",
    "\n",
    "# If some articles are redirections, we'll exclude them from our set of articles\n",
    "redirections = []\n",
    "articles_content = dict()\n",
    "counter = 0\n",
    "articles_found = []\n",
    "articles_wikitexts = []\n",
    "outdir_wikitexts = './outputs/wikivitals_raw_wikitexts/'\n",
    "wikiid_title_file = './outputs/wikiid_title.txt'\n",
    "g = open(wikiid_title_file, 'w', encoding='utf8')\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    for id, title, redirect, wikitext in mwxml.map(page_info, [file]):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Exclude redirections \n",
    "        if not redirect == None:\n",
    "            redirections.append(title)\n",
    "        else:\n",
    "    \n",
    "            # if (not title in articles_content.keys()) or (articles_content[title]['clean abstract'] == None):\n",
    "            articles_content[title] = {\n",
    "                'id': id,\n",
    "                'title': title,\n",
    "                'class0': articles_classification[title]['class0'],\n",
    "                'class1': articles_classification[title]['class1'],\n",
    "                'class2': articles_classification[title]['class2']\n",
    "            }\n",
    "            articles_found.append(title)\n",
    "            # articles_wikitexts.append(wikitext)\n",
    "            counter +=1\n",
    "            if counter%1000 == 0:\n",
    "                print(f'{counter} articles found')\n",
    "            filename = outdir_wikitexts + f'{id}.wt' \n",
    "            f = open(filename, 'w', encoding='utf8')\n",
    "            f.write(wikitext)\n",
    "            f.close()\n",
    "            g.write(f'{id}\\t{title}\\n')\n",
    "g.close()\n",
    "\n",
    "print(len(list(articles_content.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract abstracts, headers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles treated\n",
      "2000 articles treated\n",
      "3000 articles treated\n",
      "4000 articles treated\n",
      "5000 articles treated\n",
      "6000 articles treated\n",
      "7000 articles treated\n",
      "8000 articles treated\n",
      "9000 articles treated\n",
      "10000 articles treated\n",
      "11000 articles treated\n",
      "12000 articles treated\n",
      "13000 articles treated\n",
      "14000 articles treated\n",
      "15000 articles treated\n",
      "16000 articles treated\n",
      "17000 articles treated\n",
      "18000 articles treated\n",
      "19000 articles treated\n",
      "20000 articles treated\n",
      "21000 articles treated\n",
      "22000 articles treated\n",
      "23000 articles treated\n",
      "24000 articles treated\n",
      "25000 articles treated\n",
      "26000 articles treated\n",
      "27000 articles treated\n",
      "28000 articles treated\n",
      "29000 articles treated\n",
      "30000 articles treated\n",
      "31000 articles treated\n",
      "32000 articles treated\n",
      "33000 articles treated\n",
      "34000 articles treated\n",
      "35000 articles treated\n",
      "36000 articles treated\n",
      "37000 articles treated\n",
      "38000 articles treated\n",
      "39000 articles treated\n",
      "40000 articles treated\n",
      "41000 articles treated\n",
      "42000 articles treated\n",
      "43000 articles treated\n",
      "44000 articles treated\n",
      "45000 articles treated\n",
      "46000 articles treated\n",
      "47000 articles treated\n",
      "48000 articles treated\n"
     ]
    }
   ],
   "source": [
    "import mwxml\n",
    "import mwtypes\n",
    "import wikitextparser\n",
    "import mwparserfromhell\n",
    "from bs4 import BeautifulSoup\n",
    "# Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "\n",
    "outdir_wikitexts = './outputs/wikivitals_raw_wikitexts/'\n",
    "# outdir_wikitexts = './outputs/wikivitals_manual_changes/'\n",
    "wikitext_files = os.listdir(outdir_wikitexts)\n",
    "files = [outdir_wikitexts + f for f in wikitext_files]\n",
    "outfile_headers = open('./outputs/__headers.txt', 'w', encoding='utf8')\n",
    "outfile_links = open('./outputs/__links.txt', 'w',encoding='utf8')\n",
    "# outfile_abstracts = open('./outputs/__abstracts.txt', 'w', encoding='utf8')\n",
    "\n",
    "raw_abstract_kept = [] # store the list of articles the complete wikitext has been kept as an abstract\n",
    "\n",
    "# # adjustment1 = re.compile(r'thumb\\|.*?\\n')\n",
    "# adjustment2 = re.compile(r'\\{\\|.*\\|\\}', re.DOTALL)\n",
    "# adjustment3 = re.compile(r'\\[\\[.*\\]\\]', re.DOTALL)\n",
    "# references = re.compile(r'<ref.*?</ref>', re.DOTALL)\n",
    "# # adjustment4 = re.compile(r'^.*\\|.*?\\n', re.DOTALL)\n",
    "\n",
    "# namespaces = ['Talk:','User:','User talk:', 'Wikipedia:','Wikipedia talk:',\n",
    "#     'File:','File talk:','MediaWiki:','MediaWiki talk:','Template:','Template talk:',\n",
    "#     'Help:','Help talk:','Category:','Category talk:','Portal:','Portal talk:','Draft:',\n",
    "#     'Draft talk:','TimedText:','TimedText talk:','Module:','Module talk:', 'Image:']\n",
    "# namespaces = namespaces + [i.lower() for i in namespaces]\n",
    "# wikiprojects = ['wikt:', 'Wiktionary:', 's:', 'wikisource:', 'w:', 'iarchive:', 'b:']\n",
    "# wikiprojects = wikiprojects + [i.lower() for i in wikiprojects]\n",
    "# languages = ['de:', 'fr:', 'nl:', 'it:', 'da:', 'ja:', 'pl:'] \n",
    "# others = ['doi:']\n",
    "# tmp = namespaces + wikiprojects + languages + others \n",
    "# prefixes = tmp + [':' + i for i in tmp]\n",
    "\n",
    "\n",
    "\n",
    "# def remove_first_root_templates(wikicode):\n",
    "#     new_wikicode = wikicode.strip()\n",
    "#     if not new_wikicode.startswith('{'):\n",
    "#         return new_wikicode\n",
    "#     if new_wikicode.startswith('{{'):\n",
    "#         s_start, s_end = '{{', '}}'\n",
    "#     if node_str.startswith('{|class=\"wikitable\"'):\n",
    "#         s_start, s_end = '{|class=\"wikitable\"', '|}'\n",
    "#     length = len(new_wikicode)\n",
    "#     index = 0\n",
    "#     counter = 0\n",
    "#     while index < length:\n",
    "\n",
    "# def replace_foreign_language(soup):\n",
    "#     # See the templates here: https://en.wikipedia.org/wiki/Template:Nihongo\n",
    "#     # {{Nihongo|<english>|<kanji/kana>|<rōmaji>|lead=yes|extra=<extra>|extra2=<extra2>}}\n",
    "#     psoup = soup\n",
    "#     tagid = 'xmltemplate'\n",
    "#     # get the list of all tags ordered by \"length\"\n",
    "#     # links are ordered in order to replace first the link inside links\n",
    "#     # and don't miss one in the list when files are removed\n",
    "#     ordered_tags = psoup.find_all(tagid)\n",
    "#     ordered_tags = sorted(ordered_tags, key=lambda x: len(str(x)))\n",
    "\n",
    "#     for l in ordered_tags:\n",
    "#         # Japanese words\n",
    "#         if l.text.startswith(('Nihongo|', 'nihongo|')):\n",
    "#             try:\n",
    "#                 tmp = l.text.split('|')\n",
    "#                 romaji = tmp[1]\n",
    "#                 if len(tmp) > 3:\n",
    "#                     tmp = tmp[4:]\n",
    "#                     tmp = [t.split('=')[-1] for t in tmp]\n",
    "#                     tmp = [romaji] + tmp\n",
    "#                 else:\n",
    "#                     tmp = [romaji]\n",
    "#                 l.replace_with(' '.join(tmp))\n",
    "#             except:\n",
    "#                 l.replace_with(' ')\n",
    "#         elif l.text.startswith('lang|'):\n",
    "#             tmp = l.text.split('|')\n",
    "#             if len(tmp) >= 3:\n",
    "#                 replacement_text = tmp[2]\n",
    "#                 # print(f'wtext {l.text} replaced by {replacement_text}')\n",
    "#                 l.replace_with(replacement_text)\n",
    "                \n",
    "#         elif l.text.startswith('lang-'):\n",
    "#             tmp = l.text.split('|')\n",
    "#             if len(tmp) >= 2:\n",
    "#                 replacement_text = tmp[1]\n",
    "#                 # print(f'wtext {l.text} replaced by {replacement_text}')\n",
    "#                 l.replace_with(replacement_text)\n",
    "#         else:\n",
    "#             None\n",
    "        \n",
    "#     return psoup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def removeHTMLTaggedContent(soup, listOfTags = []):\n",
    "    psoup = soup\n",
    "    for tagid in listOfTags:\n",
    "        # get the list of all tags ordered by \"length\"\n",
    "        # links are ordered in order to replace first the link inside links\n",
    "        # and don't miss one in the list when files are removed\n",
    "        ordered_tags = psoup.find_all(tagid)\n",
    "        ordered_tags = sorted(ordered_tags, key=lambda x: len(str(x)))\n",
    "\n",
    "        for l in ordered_tags:\n",
    "            l.decompose()\n",
    "    return psoup\n",
    "        \n",
    "def updateInternalLinks(soup, use_replacement_text =  True):\n",
    "    psoup = soup\n",
    "    # get the list of all internal links ordered by \"length\"\n",
    "    # links are ordered in order to replace first the link inside links\n",
    "    # and don't miss one in the list when files are removed\n",
    "    count_links_replaced, count_links_removed = 0, 0\n",
    "    ordered_links = psoup.find_all('internallink')\n",
    "    ordered_links = sorted(ordered_links, key=lambda x: len(str(x)))\n",
    "    #Internal links follow this template:\n",
    "    #<internallink> canonical page name | page name in context </internallink>\n",
    "    #some links are prefixed with \"category:\", such prefixes are removed\n",
    "    for l in ordered_links:\n",
    "        if l.text.startswith(tuple(namespaces)):\n",
    "            l.decompose()\n",
    "            count_links_removed\n",
    "        else:\n",
    "            if use_replacement_text:\n",
    "                canonical_link = l.text.split('|')[-1]\n",
    "                # print(f'wtext {l.text} replaced by {canonical_link}')\n",
    "            else:\n",
    "                canonical_link = l.text.split('|')[0]\n",
    "                # replace the tag by the canonical form or\n",
    "                # remove the tag if it refers to a page in another namespace, \n",
    "                # another wiki project, or another language\n",
    "                # if not canonical_link.startswith(tuple(prefixes)):\n",
    "            l.replace_with(canonical_link) \n",
    "            count_links_replaced +=1\n",
    "\n",
    "    return psoup, count_links_replaced, count_links_removed\n",
    "\n",
    "\n",
    "# {{short description|Equation of statistical mechanics}}\n",
    "# {{other uses|Boltzmann's entropy formula|Stefan–Boltzmann law|Maxwell–Boltzmann distribution}}\n",
    "# {{redirect|BTE}}\n",
    "\n",
    "# [[File:StairsOfReduction.svg|thumb|The place of the Boltzmann kinetic equation on the stairs of model reduction from microscopic dynamics to macroscopic continuum dynamics (illustration to the content of the book<ref>\n",
    "# {{cite book |last1=Gorban |first1= Alexander N.|last2= Karlin |first2= Ilya V. |date=2005 |title= Invariant Manifolds for Physical and Chemical Kinetics|url= https://www.academia.edu/17378865| location= Berlin, Heidelberg |publisher= Springer|series= Lecture Notes in Physics (LNP, vol. 660)| isbn= 978-3-540-22684-0|doi= 10.1007/b98103}} [https://archive.org/details/gorban-karlin-lnp-2005 Alt URL]</ref>)]]\n",
    "\n",
    "\n",
    "def process_abstract(wikitext, first_try = True):\n",
    "    ptext = wikitext\n",
    "\n",
    "\n",
    "    # see = False\n",
    "    # if ptext.startswith('{{short description|Probability distribution and special case of gamma distribution}}'):\n",
    "    #     see = True\n",
    "\n",
    "\n",
    "\n",
    "    # removal of math equations before the replacement of \n",
    "    # {{ and }} tags (these tags can be found inside some\n",
    "    # equations and introduce exceptions)\n",
    "    ptext = ptext.replace('/>', ' />')\n",
    "    psoup = BeautifulSoup(ptext, 'html.parser')\n",
    "    psoup = removeHTMLTaggedContent(psoup,['math'])\n",
    "    ptext = str(psoup)\n",
    "\n",
    "    # if see:\n",
    "    #     print(ptext)\n",
    "    #replacement of some tags by HTML-like tags\n",
    "    #such tags are handled like any other tags by Beautiful soup\n",
    "    # starbox_begin = re.search('{{Starbox begin.*?}}', ptext)\n",
    "    # starbox_end = re.search('{{Starbox end.*?}}', ptext)\n",
    "    # if not starbox_begin == None and not starbox_end == None:\n",
    "    #     ptext = ptext.replace(starbox_begin.group(0), '<xmltemplate>')\n",
    "    #     ptext = ptext.replace(starbox_end.group(0), '</xmltemplate>')\n",
    "    #     print(\"starbox detected\")\n",
    "    \n",
    "    # #### VERY SPECIFIC TO WIKIPEDIA (MANUAL CHECKING)\n",
    "    # #### IN SOME PLACES, THERE ARE LONELY '}}' SUBSTRINGS. I REMOVE THEM \n",
    "    # ptext = ptext.replace('(enter DEATH date then BIRTH date (e.g., ...|1908|31|8|1967|28|2}}', ' ') # for many articles (an example)\n",
    "    # ptext = ptext.replace('<ref name=\"sd.news.cn\"> url=http://sd.news.cn/news/2022-01/20/c_1128280781.htm}}</ref>', ' ') # for article 105032\n",
    "    # # <ref name=\"jswx.gov.cn\"> url=https://www.jswx.gov.cn/chuanbo/ping/202201/t20220120_2933372.shtml}}</ref>\n",
    "    \n",
    "    # First, let's remove notes and refs BEFORE replacing {{ and }} substrings\n",
    "    # (in some notes or references, there are }} without any corresponding {{ before)\n",
    "    ptext = ptext.replace('<!--', '<note>')\n",
    "    ptext = ptext.replace('-->', '</note>')\n",
    "    psoup = BeautifulSoup(ptext, 'html.parser')\n",
    "    psoup = removeHTMLTaggedContent(\n",
    "        psoup,\n",
    "        [\n",
    "        'ref',\n",
    "        'note',\n",
    "        'gallery', # galleries of images\n",
    "        'imagemap'\n",
    "        ])\n",
    "    ptext = str(psoup)\n",
    "    # Note: there are some articles with errors in the wikitext. \n",
    "    # For example, article 11049 (FIFA) has a note without ending (<!-- but no -->)\n",
    "    # These cases have been handled MANUALLY\n",
    "\n",
    "    # if see:\n",
    "    #     print(ptext)\n",
    "\n",
    "    # Second, we handle the templates\n",
    "    ptext = ptext.replace('{{', '<xmltemplate>')\n",
    "    ptext = ptext.replace('}}', '</xmltemplate>')\n",
    "    ptext = ptext.replace('{|', '<xmltemplate>')\n",
    "    ptext = ptext.replace('|}', '</xmltemplate>')\n",
    "\n",
    "    ptext = ptext.replace('[[', '<internallink>')\n",
    "    ptext = ptext.replace(']]', '</internallink>')\n",
    "    # update_h_tags()\n",
    "\n",
    "\n",
    "    # soup = BeautifulSoup(ptext, 'html.parser')\n",
    "    psoup = BeautifulSoup(ptext, 'html.parser')\n",
    "    # psoup = replace_foreign_language(psoup)\n",
    "    #From here, only pbsoup is updated\n",
    "    psoup, _, _ = updateInternalLinks(psoup)\n",
    "    # print(self.ptext[:5000])\n",
    "    # print(\"------------------------\")\n",
    "    # print(\"------------------------\")\n",
    "    # print(\"------------------------\")\n",
    "    # print(\"------------------------\")\n",
    "    # print(self.psoup)\n",
    "    psoup = removeHTMLTaggedContent(\n",
    "        psoup,\n",
    "        [\n",
    "        'xmltemplate'\n",
    "        ])\n",
    "\n",
    "    abstract = psoup.get_text()\n",
    "    #let's remove URLs\n",
    "    abstract = re.sub('https{,1}://.*? ', ' ', abstract, flags=re.DOTALL)\n",
    "    #let's remove all characters except letters, digits, and whitespaces\n",
    "    abstract = re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",abstract) # I keep numbers, letters (accentuated letters included)\n",
    "    abstract = ' '.join(abstract.split())\n",
    "    abstract = abstract.strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    # IF ABSTRACT IS EMPTY, TRY TO SPLIT THE WIKITEXT IN 2\n",
    "    # OR APPLY A SPECIFIC TREATMENT\n",
    "    if abstract.strip() == '' and first_try:\n",
    "        # 1st step: specific treatment for some articles\n",
    "        # wikitext_mod = wikitext\n",
    "        # wikitext_mod = wikitext_mod.replace()\n",
    "\n",
    "        match = re.compile(r\"\\n\\n[^|]*?'''\", re.DOTALL)\n",
    "        a = re.search(match, wikitext)\n",
    "        if not a == None:\n",
    "            a = a.group(0)\n",
    "            wikitext_cut = wikitext.split(a, 1)[-1]\n",
    "            # parts = [process_abstract(w, False) for w in wikitext_cut]\n",
    "            # abstract = ' '.join(parts)\n",
    "            # abstract = wikitextparser.parse(wikitext).plain_text()\n",
    "            abstract = process_abstract(wikitext_cut, first_try = False)\n",
    "            print(abstract)\n",
    "\n",
    "    return abstract\n",
    "        \n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    id = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    wikitext = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Find highest level headers (h2 to h5)\n",
    "    headers = get_highest_level_headers(wikitext, 2)\n",
    "    headers_str = [wikitextparser.remove_markup(h.title) for h in headers]\n",
    "    outfile_headers.write(f'{id}\\t{\" §§ \".join(headers_str)}\\n')\n",
    "\n",
    "    # Find the internal links\n",
    "    links, _ = wikilinks_namespace0(wikitext)\n",
    "    links = list(set(links))\n",
    "    links = [l.strip() for l in links if not l == '']\n",
    "    outfile_links.write(f'{id}\\t{\" §§ \".join(links)}\\n')\n",
    "    \n",
    "\n",
    "    # # SLOW WAY TO EXTRACT THE HEADING PART\n",
    "    # wikicode = mwparserfromhell.parse(wikitext)\n",
    "    # raw_abstract = str(wikicode.get_sections(include_lead=True, include_headings=False)[0])\n",
    "    # # print(raw_abstract)\n",
    "\n",
    "\n",
    "    # # Abstract extraction\n",
    "    # try:\n",
    "    #     # # Split text at the first header\n",
    "    #     # size = max(100, len(str(headers[0])))\n",
    "    #     # first_header = str(headers[0])[:size]\n",
    "    #     # raw_abstract =  wikitext.split(first_header)[0]\n",
    "    #     first_header = str(headers[0])\n",
    "    #     raw_abstract =  wikitext.split(first_header)[0]\n",
    "    \n",
    "    # except:\n",
    "    #     # Keep the raw text if no header\n",
    "    #     raw_abstract = wikitext\n",
    "    #     raw_abstract_kept.append(id)\n",
    "    #     print(id)\n",
    "\n",
    "    # clean_abstract = process_abstract(raw_abstract)\n",
    "    # outfile_abstracts.write(f'{id}\\t{clean_abstract}\\n')\n",
    "\n",
    "    counter += 1\n",
    "    if counter%1000 == 0:\n",
    "        print(f\"{counter} articles treated\")\n",
    "\n",
    "outfile_headers.close()\n",
    "outfile_links.close()\n",
    "# outfile_abstracts.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll filter articles based on abstracts and more\n",
    "\n",
    "\n",
    "Output exceeds the size limit. Open the full output data in a text editor\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles1.xml-p1p41242.bz2\n",
    "1000 articles found\n",
    "2000 articles found\n",
    "3000 articles found\n",
    "4000 articles found\n",
    "5000 articles found\n",
    "6000 articles found\n",
    "7000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles10.xml-p4045403p5399366.bz2\n",
    "8000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p5399367p6899366.bz2\n",
    "9000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p6899367p7054859.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p7054860p8554859.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p8554860p9172788.bz2\n",
    "10000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p10672789p11659682.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p9172789p10672788.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p11659683p13159682.bz2\n",
    "11000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p13159683p14324602.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p14324603p15824602.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p15824603p17324602.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p17324603p17460152.bz2\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p17460153p18960152.bz2\n",
    "...\n",
    "47000 articles found\n",
    "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles9.xml-p2936261p4045402.bz2\n",
    "48000 articles found\n",
    "48512\n",
    "\n",
    "\n",
    "Number of articles in hierarchy: 48649\n",
    "Number of articles recovered: 48596\n",
    "Number of duplicates in the hierarchy: 53\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Output exceeds the size limit. Open the full output data in a text editor\n",
    "10630377\n",
    "1000 articles treated\n",
    "FIFA''' ( ; ; Spanish: ''Federación Internacional de Fútbol Asociación''; German: ''Internationaler Verband des Association-Fußball''; Russian: ''Международная федерация футбола''; Arabic: ''الاتحاد الدولي لكرة القدم'') is a non-profit organization that describes itself as an international governing body of association football, futsal and beach soccer. It is the highest governing body of association football. FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, Denmark, France, Germany, the Netherlands, Spain, Sweden and Switzerland. Headquartered in Zürich, Switzerland, its membership now comprises 211 national associations; Russia was suspended in 2022. These national associations must each also be members of one of the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America and the Caribbean, Oceania and South America. Today, FIFA outlines a number of objectives in the organizational Statutes, including growing association football internationally, providing efforts to ensure it is accessible to everyone, and advocating for integrity and fair play. FIFA is responsible for the organization and promotion of association football's major international tournaments, notably the World Cup which commenced in 1930 and the Women's World Cup which commenced in 1991. Although FIFA does not solely set the laws of the game, that being the responsibility of the International Football Association Board of which FIFA is a member, it applies and enforces the rules across all FIFA competitions. All FIFA tournaments generate revenue from sponsorship; in 2018, FIFA had revenues of over US $4.6 billion, ending the 2015–2018 cycle with a net positive of US$1.2 billion, and had cash reserves of over US$2.7 billion. Reports by investigative journalists have linked FIFA leadership with corruption, bribery, and vote-rigging related to the election of FIFA president Sepp Blatter and the organization's decision to award the 2018 and 2022 World Cups to Russia and Qatar, respectively. These allegations led to the indictments of nine high-ranking FIFA officials and five corporate executives by the U.S. Department of Justice on charges including racketeering, wire fraud, and money laundering. On 27 May 2015, several of these officials were arrested by Swiss authorities, who were launching a simultaneous but separate criminal investigation into how the organization awarded the 2018 and 2022 World Cups. Those among these officials who were also indicted in the U.S. are expected to be extradited to face charges there as well. Many officials were suspended by FIFA's ethics committee including Sepp Blatter and Michel Platini. In early 2017, reports became public about FIFA president Gianni Infantino attempting to prevent the re-elections of both chairmen of the ethics committee, Cornel Borbély and Hans-Joachim Eckert, during the FIFA congress in May 2017. On 9 May 2017, following Infantino's proposal, FIFA Council decided not to renew the mandates of Borbély and Eckert. Together with the chairmen, 11 of 13 committee members were removed.\n",
    "chi-squared distribution''' (also '''chi-square''' or ) with degrees of freedom is the distribution of a sum of the squares of independent standard normal random variables. The chi-squared distribution is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing and in construction of confidence intervals. This distribution is sometimes called the '''central chi-squared distribution''', a special case of the more general noncentral chi-squared distribution. The chi-squared distribution is used in the common chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. Many other statistical tests also use this distribution, such as Friedman's analysis of variance by ranks.\n",
    "2000 articles treated\n",
    "3000 articles treated\n",
    "4000 articles treated\n",
    "5000 articles treated\n",
    "Abū Muḥammad ʿAlī ibn Aḥmad ibn Saʿīd ibn Ḥazm''' (; also sometimes known as al-Andalusī aẓ-Ẓāhirī; 7 November 994 – 15 August 1064 [456 AH]) was an Andalusian Muslim polymath, historian, jurist, philosopher, and theologian, born in the Caliphate of Córdoba, present-day Spain. Described as one of the strictest hadith interpreters, Ibn Hazm was a leading proponent and codifier of the Zahiri school of Islamic thought and produced a reported 400 works, of which only 40 still survive. In all, his written works amounted to some 80 000 pages. Described as one of the fathers of comparative religion, the ''Encyclopaedia of Islam'' refers to him as having been one of the leading thinkers of the Muslim world.\n",
    "6000 articles treated\n",
    "7000 articles treated\n",
    "8000 articles treated\n",
    "Alopecia areata''', also known as '''spot baldness''', is a condition in which hair is lost from some or all areas of the body. Often, it results in a few bald spots on the scalp, each about the size of a coin. Psychological stress and illness are possible factors in bringing on alopecia areata in individuals at risk, but in most cases there is no obvious trigger. People are generally otherwise healthy. In a few cases, all the hair on the scalp is lost (''alopecia totalis''), or all body hair is lost (''alopecia universalis''), and loss can be permanent. It is distinctive from pattern hair loss, which is common among males. Alopecia areata is believed to be an autoimmune disease resulting from a breach in the immune privilege of the hair follicles. Risk factors include a family history of the condition. Among identical twins, if one is affected, the other has about a 50% chance of also being affected. The underlying mechanism involves failure by the body to recognize its own cells, with subsequent immune-mediated destruction of the hair follicle. No cure for the condition is known. Efforts may be used to try to speed hair regrowth, such as cortisone injections. Sunscreen, head coverings to protect from cold and sun, and glasses, if the eyelashes are missing, are recommended. In some cases, the hair regrows, and the condition does not reoccur. In others, hair loss and regrowth occurs over years. Among those in whom all body hair is lost, fewer than 10% recover. About 0.15% of people are affected at any one time, and 2% of people are affected at some point in time. Onset is usually in childhood. Males and females have the condition in equal numbers. The condition does not affect a person's life expectancy.\n",
    "9000 articles treated\n",
    "1714180\n",
    "10000 articles treated\n",
    "11000 articles treated\n",
    "12000 articles treated\n",
    "197245\n",
    "13000 articles treated\n",
    "2011918\n",
    "20412550\n",
    "20412640\n",
    "14000 articles treated\n",
    "2053318\n",
    "...\n",
    "46000 articles treated\n",
    "Water vapor''', '''water vapour''' or '''aqueous vapor''' is the gaseous phase of water. It is one state of water within the hydrosphere. Water vapor can be produced from the evaporation or boiling of liquid water or from the sublimation of ice. Water vapor is transparent, like most constituents of the atmosphere. Under typical atmospheric conditions, water vapor is continuously generated by evaporation and removed by condensation. It is less dense than most of the other constituents of air and triggers convection currents that can lead to clouds. Being a component of Earth's hydrosphere and hydrologic cycle, it is particularly abundant in Earth's atmosphere, where it acts as a greenhouse gas and warming feedback, contributing more to total greenhouse effect than non-condensable gases such as carbon dioxide and methane. Use of water vapor, as steam, has been important for cooking, and as a major component in energy production and transport systems since the industrial revolution. Water vapor is a relatively common atmospheric constituent, present even in the solar atmosphere as well as every planet in the Solar System and many astronomical objects including natural satellites, comets and even large asteroids. Likewise the detection of extrasolar water vapor would indicate a similar distribution in other planetary systems. Water vapor is significant in that it can be indirect evidence supporting the presence of extraterrestrial liquid water in the case of some planetary mass objects.\n",
    "47000 articles treated\n",
    "48000 articles treated\n",
    "48512 articles\n",
    "0 articles with the same name\n",
    "Abstracts:\n",
    "* Average abstract size: 1365.9486518799472 (1054.3838368074944)\n",
    "* 12 empty abstracts\n",
    "* 26 (0.05%) short abstracts (less than 50 characters, can be considered empty):\n",
    "  ['Crate', 'Chi-squared distribution', 'Sono Osato', 'Lois Mailou Jones', 'Timeline of zoology', 'Alopecia areata', 'History of Hesse', 'Ahmed Hassan al-Bakr', 'Kim Young-ha', 'Law of Austria', 'Arthropathy', 'History of Saint Vincent and the Grenadines', 'History of the jet engine', 'Kim Dong-in', 'Lists of legislation', 'History of Vanuatu', 'West Bengal', 'Road junction', 'Timeline of railway history', 'History of Benin', 'Belgrade', 'Timeline of geology', 'Law of Denmark', 'Districts of Suriname', 'Planter', 'Water vapor']\n",
    "\n",
    "Headers:\n",
    "* 20 articles with no header found:\n",
    "  ['Stratigraphic section', 'Controller (computing)', 'South East Point', 'Minoo Island', 'Elbazduko Britayev', 'Seka Gadiyev', 'Robert Guérin', 'Glass lizard', 'Authentication (law)', 'Muhammad Imaaduddeen IV', 'Recrystallization (geology)', 'Tayabas Isthmus', 'Trade journalism', 'Distribution function (physics)', 'Chemical law', 'Climate change and agriculture', 'Mark (unit)', 'North–South divide', 'Hum Log', 'Egyptian law']\n",
    "\n",
    "There are 0 canonical names that startswith a lowercase letter\n",
    "4 pages with no outgoing edge\n",
    "2297586 directed edges found\n",
    "11 classes level 0: [1126, 1408, 2422, 2979, 3148, 3310, 4255, 4290, 4681, 5318, 15575] (48512)\n",
    "32 classes level 1: [355, 360, 500, 608, 791, 849, 886, 886, 988, 1012, 1108, 1126, 1186, 1191, 1207, 1210, 1231, 1335, 1386, 1408, 1825, 1902, 2030, 2075, 2120, 2310, 2342, 2396, 2452, 2979, 3148, 3310] (48512)\n",
    "251 classes level 2: [1, 2, 3, 3, 3, 5, 5, 5, 6, 8, 8, 9, 10, 10, 10, 10, 10, 11, 12, 13, 14, 15, 15, 17, 17, 17, 17, 18, 18, 19, 20, 20, 20, 22, 23, 25, 25, 25, 29, 30, 30, 35, 35, 35, 35, 36, 36, 36, 37, 39, 39, 39, 40, 40, 40, 40, 40, 40, 41, 42, 42, 43, 43, 43, 44, 45, 45, 47, 47, 48, 48, 49, 50, 50, 51, 51, 51, 51, 52, 52, 52, 53, 54, 55, 56, 56, 57, 58, 59, 60, 60, 60, 60, 60, 61, 62, 63, 68, 69, 69, 69, 70, 71, 72, 72, 74, 74, 78, 78, 78, 79, 81, 84, 85, 85, 86, 88, 89, 89, 89, 89, 90, 91, 92, 93, 93, 94, 97, 104, 105, 109, 110, 110, 110, 111, 111, 114, 116, 117, 120, 121, 121, 122, 128, 129, 131, 132, 134, 136, 138, 139, 139, 140, 140, 147, 149, 151, 152, 160, 160, 167, 170, 172, 172, 175, 177, 185, 192, 194, 194, 196, 197, 198, 199, 200, 201, 201, 204, 219, 220, 221, 221, 226, 226, 230, 234, 237, 238, 241, 249, 250, 250, 267, 270, 271, 273, 275, 280, 295, 297, 298, 299, 300, 301, 302, 302, 303, 323, 338, 353, 357, 365, 369, 378, 382, 383, 387, 395, 411, 420, 440, 441, 451, 470, 490, 500, 501, 503, 518, 529, 539, 542, 546, 563, 567, 594, 629, 654, 668, 741, 803, 807, 825, 865, 873, 931, 989, 1146, 1424, 1503, 1725] (48512)\n",
    "Classes (level 2) with less than 3 nodes: ['People ->- Miscellaneous ->- Micronations', 'Physical sciences ->- Earth science ->- Earth science basics']\n",
    "Nodes in classes that have less than 3 nodes: ['31749258', '2890783', '20653168']\n",
    "Wikipedia:Vital articles/Level/5/People/Artists, musicians, and composers:{'class0': 'People', 'class1': 'Artists, musicians, and composers'}\n",
    "Output exceeds the size limit. Open the full output data in a text editor\n",
    "Wikipedia:Vital articles/Level/5/People/Writers and journalists\n",
    "Wikipedia:Vital articles/Level/5/People/Artists, musicians, and composers\n",
    "Wikipedia:Vital articles/Level/5/People/Entertainers, directors, producers, and screenwriters\n",
    "Wikipedia:Vital articles/Level/5/People/Philosophers, historians, political and social scientists\n",
    "Wikipedia:Vital articles/Level/5/People/Religious figures\n",
    "Wikipedia:Vital articles/Level/5/People/Politicians and leaders\n",
    "Wikipedia:Vital articles/Level/5/People/Military personnel, revolutionaries, and activists\n",
    "Wikipedia:Vital articles/Level/5/People/Scientists, inventors, and mathematicians\n",
    "Wikipedia:Vital articles/Level/5/People/Sports figures\n",
    "Wikipedia:Vital articles/Level/5/People/Miscellaneous\n",
    "Wikipedia:Vital articles/Level/5/History\n",
    "Wikipedia:Vital articles/Level/5/Geography/Physical\n",
    "Wikipedia:Vital articles/Level/5/Geography/Countries\n",
    "Wikipedia:Vital articles/Level/5/Geography/Cities\n",
    "Wikipedia:Vital articles/Level/5/Arts\n",
    "Wikipedia:Vital articles/Level/5/Philosophy and religion\n",
    "Wikipedia:Vital articles/Level/5/Everyday life\n",
    "Wikipedia:Vital articles/Level/5/Everyday life/Sports, games and recreation\n",
    "Wikipedia:Vital articles/Level/5/Society and social sciences/Social studies\n",
    "Wikipedia:Vital articles/Level/5/Society and social sciences/Politic and economic\n",
    "Wikipedia:Vital articles/Level/5/Society and social sciences/Culture\n",
    "Wikipedia:Vital articles/Level/5/Biological and health sciences/Biology\n",
    "Wikipedia:Vital articles/Level/5/Biological and health sciences/Animals\n",
    "Wikipedia:Vital articles/Level/5/Biological and health sciences/Plants\n",
    "Wikipedia:Vital articles/Level/5/Biological and health sciences/Health\n",
    "...\n",
    "\n",
    "\n",
    "32 Vital articles (level 5) found\n",
    "Wiki IDs have been stored\n",
    "55702953\n",
    "13086 features calculated for abstracts\n",
    "1958 features calculated for headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmed Hassan al Bakr 1 July 1914 4 October 1982 was the fourth president of Iraq from 17 July 1968 to 16 July 1979. He was a leading member of the revolutionary Arab Socialist Ba ath Party and later the Baghdad based Ba ath Party and its regional organisation Ba ath Party Iraq Region the Ba ath Party s Iraqi branch which espoused Ba athism a mix of Arab nationalism and Arab socialism. Al Bakr first rose to prominence after the 14 July Revolution which overthrew the monarchy. In the newly established government he was involved in improving Iraqi Soviet relations. In 1959 al Bakr was forced to resign from the Iraqi military the then Iraqi government accused him of anti government activities. Following his forced retirement he became the chairman of the Ba ath Party s Iraqi branch s Military Bureau. Through this office he recruited members to the Ba athist cause through patronage and cronyism. Prime Minister Abd al Karim Qasim was overthrown in the Ramadan 8 February Revolution al Bakr was appointed Prime Minister and later Vice President of Iraq in a Ba ath Nasserist coalition government. The government lasted for less than a year and was ousted in November 1963. Al Bakr and the party then pursued underground activities and became vocal critics of the government. During this period al Bakr was elected the Ba ath Party s Iraqi branch s Secretary General the head and appointed his cousin Saddam Hussein the party cell s deputy leader. Al Bakr and the Ba ath Party regained power in the coup of 1968 later called the 17 July Revolution. In the coup s aftermath he was elected the chairman of the Revolutionary Command Council and the president he was later appointed the prime minister. Saddam the Ba ath Party s deputy became the deputy chairman of the Revolutionary Command Council and vice president and was responsible for Iraq s security services. Under al Bakr s rule Iraq grew economically due to high international oil prices which strengthened its position in the Arab world and increased Iraqis standard of living. Land reforms were introduced and wealth was distributed more equally. A sort of socialist economy was established in the late 1970s under Saddam s direction. Al Bakr gradually lost power to Saddam in the 1970s as the latter strengthened his position within the party and the state through security services. In 1979 al Bakr resigned from all public offices for health reasons . He died in 1982 of unreported causes.\n",
      "\n",
      "\n",
      "Vladivostok is the largest city and the administrative centre of Primorsky Krai Russia. The city is located around the Golden Horn Bay on the Sea of Japan covering an area of with a population of 600 871 residents as of 2021. Vladivostok is the second largest city in the Far Eastern Federal District as well as the Russian Far East after Khabarovsk. The city was founded in 1860 as a Russian military outpost. In 1872 the main Russian naval base on the Pacific Ocean was transferred to the city and thereafter Vladivostok began to grow. After the outbreak of the Russian Revolution in 1917 Vladivostok was occupied in 1918 by foreign troops the last of whom from Japan were not withdrawn until 1922 by that time the anti revolutionary White Army forces in Vladivostok promptly collapsed and Soviet power was established in the city. After the dissolution of the Soviet Union Vladivostok became the administrative centre of Primorsky Krai. Vladivostok is the largest Russian port on the Pacific Ocean and the chief economic scientific and cultural center of the Russian Far East as well as an important tourism centre in Russia. As the terminus of the Trans Siberian Railway the city was visited by over three million tourists in 2017. The city is the administrative center of the Far Eastern Federal District and is the home to the headquarters of the Pacific Fleet of the Russian Navy. For its unique geographical location and its Russian culture the city has been called Europe in the Orient . Many foreign consulates and businesses have offices in Vladivostok. With an annual mean temperature of around Vladivostok has a cold climate for its mid latitude coastal setting. This is due to winds from the vast Eurasian landmass in winter also cooling the ocean temperatures.\n",
      "Vladivostok is the largest city and the administrative centre of Primorsky Krai Russia. The city is located around the Golden Horn Bay on the Sea of Japan covering an area of with a population of 600 871 residents as of 2021. Vladivostok is the second largest city in the Far Eastern Federal District as well as the Russian Far East after Khabarovsk. The city was founded in 1860 as a Russian military outpost. In 1872 the main Russian naval base on the Pacific Ocean was transferred to the city and thereafter Vladivostok began to grow. After the outbreak of the Russian Revolution in 1917 Vladivostok was occupied in 1918 by foreign troops the last of whom from Japan were not withdrawn until 1922 by that time the anti revolutionary White Army forces in Vladivostok promptly collapsed and Soviet power was established in the city. After the dissolution of the Soviet Union Vladivostok became the administrative centre of Primorsky Krai. Vladivostok is the largest Russian port on the Pacific Ocean and the chief economic scientific and cultural center of the Russian Far East as well as an important tourism centre in Russia. As the terminus of the Trans Siberian Railway the city was visited by over three million tourists in 2017. The city is the administrative center of the Far Eastern Federal District and is the home to the headquarters of the Pacific Fleet of the Russian Navy. For its unique geographical location and its Russian culture the city has been called Europe in the Orient . Many foreign consulates and businesses have offices in Vladivostok. With an annual mean temperature of around Vladivostok has a cold climate for its mid latitude coastal setting. This is due to winds from the vast Eurasian landmass in winter also cooling the ocean temperatures.\n",
      "\n",
      "\n",
      "Salish Sea is a marginal sea of the Pacific Ocean located in the Canadian province of British Columbia and the U.S. state of Washington. It includes the Strait of Georgia Strait of Juan de Fuca Puget Sound and an intricate network of connecting channels and adjoining waterways. The sea stretches from the channels of the Discovery Islands north of the Strait of Georgia to Budd Inlet at the south end of Puget Sound. It is partially separated from the open Pacific Ocean by Vancouver Island and the Olympic Peninsula. Much of the coast is part of the Pacific Northwest megalopolis. Major port cities on the Salish Sea include Vancouver Seattle Tacoma Everett Bellingham Port Angeles and Victoria.\n",
      "Salish Sea is a marginal sea of the Pacific Ocean located in the Canadian province of British Columbia and the U.S. state of Washington. It includes the Strait of Georgia Strait of Juan de Fuca Puget Sound and an intricate network of connecting channels and adjoining waterways. The sea stretches from the channels of the Discovery Islands north of the Strait of Georgia to Budd Inlet at the south end of Puget Sound. It is partially separated from the open Pacific Ocean by Vancouver Island and the Olympic Peninsula. Much of the coast is part of the Pacific Northwest megalopolis. Major port cities on the Salish Sea include Vancouver Seattle Tacoma Everett Bellingham Port Angeles and Victoria.\n",
      "\n",
      "\n",
      "Belgrade is the capital and largest city of Serbia. It is located at the confluence of the Sava and Danube rivers and the crossroads of the Pannonian Plain and the Balkan Peninsula. Nearly 1.7 million people live within the administrative limits of the City of Belgrade. Belgrade is one of the oldest continuously inhabited cities in Europe and the World. One of the most important prehistoric cultures of Europe the Vin a culture evolved within the Belgrade area in the 6th millennium BC. In antiquity Thraco Dacians inhabited the region and after 279 BC Celts settled the city naming it Singid n . It was conquered by the Romans under the reign of Augustus and awarded Roman city rights in the mid 2nd century. It was settled by the Slavs in the 520s and changed hands several times between the Byzantine Empire the Frankish Empire the Bulgarian Empire and the Kingdom of Hungary before it became the seat of the Serbian king Stefan Dragutin in 1284. Belgrade served as capital of the Serbian Despotate during the reign of Stefan Lazarevi and then his successor ura Brankovi returned it to the Hungarian king in 1427. Noon bells in support of the Hungarian army against the Ottoman Empire during the siege in 1456 have remained a widespread church tradition to this day. In 1521 Belgrade was conquered by the Ottomans and became the seat of the Sanjak of Smederevo. It frequently passed from Ottoman to Habsburg rule which saw the destruction of most of the city during the Ottoman Habsburg wars. In the period after the Serbian Revolution Belgrade was again named the capital of Serbia in 1841. Northern Belgrade remained the southernmost Habsburg post until 1918 when it was attached to the city due to former Austro Hungarian territories becoming part of the new Kingdom of Serbs Croats and Slovenes after World War I. Belgrade was the capital of Yugoslavia from its creation in 1918 to its dissolution in 2006. In a fatally strategic position the city has been battled over in 115 wars and razed 44 times being bombed five times and besieged many times. Being Serbia s primate city Belgrade has special administrative status within Serbia. It is the seat of the central government administrative bodies and government ministries as well as home of almost all of the largest Serbian companies media and scientific institutions. Belgrade is classified as a Beta Global City. The city is home to the Clinical Centre of Serbia one of the hospital complexes with the largest capacity in the world the Church of Saint Sava one of the largest Orthodox church buildings and the tark Arena one of the indoor arenas with the largest capacity in Europe. Belgrade hosted major international events such as the Danube River Conference of 1948 the first Non Aligned Movement Summit 1961 the first major gathering of the OSCE 1977 1978 Eurovision Song Contest 2008 as well as sports events such as the first FINA World Aquatics Championships 1973 UEFA Euro 1976 Summer Universiade 2009 and EuroBasket three times 1961 1975 2005 .\n",
      "\n",
      "\n",
      "is the capital city of Ibaraki Prefecture in the northern Kant region of Japan. the city had an estimated population of 269 330 in 123 282 households and a population density of 1239 persons per km2. The percentage of the population aged over 65 was 27.1 . The total area of the city is .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outdir_wikitexts = './outputs/wikivitals_raw_wikitexts/'\n",
    "\n",
    "raw_abstract_kept = [] # store the list of articles the complete wikitext has been kept as an abstract\n",
    "\n",
    "\n",
    "# Manual treatment for some files\n",
    "ids_to_handle_manually = ['209172', '21345189', '5276276', '55904', '6793009']\n",
    "# files = [outdir_wikitexts + str(i) + '.wt' for i in ids_to_handle_manually]\n",
    "str_to_remove = {\n",
    "    '209172': ('{{efn|{{lang-ar|أحمد حسن البك}}', ' '), # problem with two '{{' but only one '}}' (parser failed)\n",
    "    '21345189': (\"\\n'''Vladivostok'''\",\"\\n\\n'''Vladivostok'''\"),\n",
    "    '5276276': (\"}}\\nThe '''Salish Sea'''\", \"}}\\n\\nThe '''Salish Sea'''\"), # problem with a comment, no \\n\\n before the first paragraph \n",
    "    '55904': ('''({{IPAc-en|b|E|l|\"|g|r|eI|d}} {{respell|bel|GRAYD}}, {{IPAc-en|'|b|ɛ|l|ɡ|ɹ|eɪ|d}} {{respell|BEL|grayd}};{{NoteTag|{{small|also}} {{IPAc-en|US|b|E|l|\"|g|r|A:|d|,_|-|\"|g|r|{|d}} {{respell|bel|GRAHD|,_|-|GRAD}}, {{IPAc-en|\"|b|E|l|g|r|A:|d|,_|-|g|r|{|d}} {{respell|BEL|grahd|,_|-|grad}}<ref>{{Cite book|title=Collins English Dictionary|publisher=HarperCollins|year=2018|isbn=0-008-28437-7|edition=13th|chapter=Belgrade}}</ref><ref>{{Cite web|title=Definition of Belgrade {{!}} Dictionary.com|url=https://www.dictionary.com/browse/Belgrade|access-date=2022-02-14|website=www.dictionary.com|language=en}}</ref><!-- 'Collins English Dictionary,' being British, does not include the pronunciation variants given in this note. -->}} {{lang-sr|Београд / Beograd|lit=White City}}, {{IPA-sh|beǒɡrad|pron|Sr-beograd-native.ogg}}; [[Names of European cities in different languages: B|names in other languages]])''', ' '), # problem with two '{{' but only one '}}' (parser failed)\n",
    "    '6793009': ('<!-- See Template:Infobox settlement for additional fields and descriptions --', '<!-- See Template:Infobox settlement for additional fields and descriptions -->') # Comment not terminated properly\n",
    "}\n",
    "\n",
    "store_content = dict()\n",
    "\n",
    "for i in ids_to_handle_manually:\n",
    "    file = outdir_wikitexts + i + '.wt'\n",
    "    id = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    wikitext = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Find highest level headers (h2 to h5)\n",
    "    headers = get_highest_level_headers(wikitext, 2)\n",
    "\n",
    "    # Abstract extraction\n",
    "    try:\n",
    "        # Split text at the first header\n",
    "        first_header = str(headers[0])\n",
    "        raw_abstract =  wikitext.split(first_header)[0]\n",
    "    except:\n",
    "        # Keep the raw text if no header\n",
    "        raw_abstract = wikitext\n",
    "        raw_abstract_kept.append(id)\n",
    "        print(id)\n",
    "\n",
    "    j,k = str_to_remove[i]\n",
    "    raw_abstract_ = raw_abstract.replace(j, k)\n",
    "\n",
    "\n",
    "    clean_abstract = process_abstract(raw_abstract_)\n",
    "    print(clean_abstract)\n",
    "    print('\\n')\n",
    "    store_content[i] = clean_abstract\n",
    "    # outfile_abstracts.write(f'{id}\\t{clean_abstract}\\n')\n",
    "\n",
    "outfile_abstracts = open('./outputs/__abstracts.txt', 'r', encoding='utf8')\n",
    "lines = outfile_abstracts.readlines()\n",
    "outfile_abstracts.close()\n",
    "\n",
    "outfile_abstracts = open('./outputs/__abstracts.txt', 'w', encoding='utf8')\n",
    "for l in lines:\n",
    "    id = l.split('\\t')[0]\n",
    "    if id in ids_to_handle_manually:\n",
    "        l_ = f'{id}\\t{store_content[id]}\\n'\n",
    "        outfile_abstracts.write(l_)\n",
    "    else:\n",
    "        outfile_abstracts.write(l)\n",
    "outfile_abstracts.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_abstracts = open('./outputs/__abstracts.txt', 'r', encoding='utf8')\n",
    "lines = outfile_abstracts.readlines()\n",
    "outfile_abstracts.close()\n",
    "\n",
    "outfile_abstracts = open('./outputs/__abstracts.txt', 'w', encoding='utf8')\n",
    "for l in lines:\n",
    "    id = l.split('\\t')[0]\n",
    "    try:\n",
    "        abstract = l.split('\\t')[1]\n",
    "        abstract = re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",abstract) # I keep numbers, letters (accentuated letters included\n",
    "        abstract = ' '.join(abstract.split())\n",
    "        l_ = f'{id}\\t{abstract}\\n'\n",
    "    except:\n",
    "        l_ = f'{id}\\t\\n'\n",
    "    outfile_abstracts.write(l_)\n",
    "outfile_abstracts.close()\n",
    "\n",
    "# abstract = re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",abstract) # I keep numbers, letters (accentuated letters included\n",
    "# abstract = ' '.join(abstract.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48512 articles\n",
      "0 articles with the same name\n",
      "Abstracts:\n",
      "* Average abstract size: 1253.1711123021107 (956.4818379403986)\n",
      "* 8 empty abstracts\n",
      "* 26 (0.05%) short abstracts (less than 50 characters, can be considered empty):\n",
      "  [('Crate', '1010583'), ('Sono Osato', '1253459'), ('Lois Mailou Jones', '1475461'), ('Timeline of zoology', '15301001'), ('Akaji Maro', '16815417'), ('History of Hesse', '182064'), ('List of fashion magazines', '20102549'), ('Kim Young-ha', '22075910'), ('Law of Austria', '25135167'), ('Arthropathy', '2571116'), ('History of Saint Vincent and the Grenadines', '27229'), ('History of the jet engine', '27888245'), ('Kim Dong-in', '28511848'), ('Lists of legislation', '32102836'), ('History of Vanuatu', '32444'), ('Isawa Shūji', '34531593'), ('Nógrád County', '349050'), ('Road junction', '3720055'), ('Law of Romania', '39338227'), ('Timeline of railway history', '398236'), ('History of Benin', '42386'), ('Timeline of materials technology', '58742'), ('Timeline of geology', '58962'), ('Law of Denmark', '6067786'), ('Districts of Suriname', '682735'), ('Planter', '6874623')]\n",
      "\n",
      "Headers:\n",
      "* 20 articles with no header found:\n",
      "  ['Stratigraphic section', 'Controller (computing)', 'South East Point', 'Minoo Island', 'Elbazduko Britayev', 'Seka Gadiyev', 'Robert Guérin', 'Glass lizard', 'Authentication (law)', 'Muhammad Imaaduddeen IV', 'Recrystallization (geology)', 'Tayabas Isthmus', 'Trade journalism', 'Distribution function (physics)', 'Chemical law', 'Climate change and agriculture', 'Mark (unit)', 'North–South divide', 'Hum Log', 'Egyptian law']\n",
      "\n",
      "There are 0 canonical names that startswith a lowercase letter\n",
      "Case Western University §§ Hacker (computer security) §§ Grand Theft Auto: Vice City §§ porm §§ Software cracking §§ newbie §§ Apheresis (linguistics) §§ Sigma Tau Delta §§ search engines §§ glyph §§ ¢ §§ Faux Cyrillic §§ Journey (band) §§ º §§ script kiddie §§ Kingdom of Loathing §§ SMS language §§ § §§ w00t §§ ° §§ teh §§ inflection §§ Sandbox (software development) §§ Warez §§ dialects §§ Role-playing game §§ Phreaking §§ hubris §§ ƒ §§ Umlaut (diacritic) §§ Internet §§ © §§ video game culture §§ Teh §§ ? §§ content filter §§ plosive §§ All your base are belong to us §§ emoticon §§ nominalization §§ verbing §§ Calculator spelling §§ Internet Relay Chat §§ slang §§ alveolar consonant §§ Internet slang §§ über §§ ASCII art §§ reflection (mathematics) §§ crustacean §§ ß §§ binary number §§ Cult of the Dead Cow §§ Jargon File §§ backronym §§ English verbs §§ pornography §§ Deadmau5 §§ prawn §§ Megatokyo §§ × §§ Scrabble §§ Hacker (hobbyist) §§ argot §§ † §§ spam (electronic) §§ B1ff §§ gamertags §§ Internet meme §§ suffixes §§ Usenet §§ Technology Review §§ ® §§ frequency analysis §§ OM Sterling Global University §§ cipher §§ parsing §§ Д §§ system operators §§ online game §§ Binary numeral system §§ instant messaging §§ Geek Code §§ homoglyph §§ elite §§ orthography §§ agent noun §§ poetry §§ User (computing) §§ Padonkaffsky jargon §§ wiktionary:über §§ Owned (slang) §§ Kevin Mitnick §§ Homestuck §§ Variety (linguistics) §§ Я §§ ampersand §§ Евро §§ warez §§ cyberculture §§ £ §§ Chi (letter) §§ bulletin board system §§ stem (linguistics) §§ palatal approximant §§ Escape_(Journey_album) §§ online communities §§ affricate\n",
      "-------------\n",
      "['Case Western University', 'Hacker (computer security)', 'Grand Theft Auto: Vice City', 'Porm', 'Software cracking', 'Newbie', 'Apheresis (linguistics)', 'Sigma Tau Delta', 'Search engines', 'Glyph', '¢', 'Faux Cyrillic', 'Journey (band)', 'º', 'Script kiddie', 'Kingdom of Loathing', 'SMS language', '§', 'W00t', '°', 'Teh', 'Inflection', 'Sandbox (software development)', 'Warez', 'Dialects', 'Role-playing game', 'Phreaking', 'Hubris', 'Ƒ', 'Umlaut (diacritic)', 'Internet', '©', 'Video game culture', 'Teh', '?', 'Content filter', 'Plosive', 'All your base are belong to us', 'Emoticon', 'Nominalization', 'Verbing', 'Calculator spelling', 'Internet Relay Chat', 'Slang', 'Alveolar consonant', 'Internet slang', 'Über', 'ASCII art', 'Reflection (mathematics)', 'Crustacean', 'SS', 'Binary number', 'Cult of the Dead Cow', 'Jargon File', 'Backronym', 'English verbs', 'Pornography', 'Deadmau5', 'Prawn', 'Megatokyo', '×', 'Scrabble', 'Hacker (hobbyist)', 'Argot', '†', 'Spam (electronic)', 'B1ff', 'Gamertags', 'Internet meme', 'Suffixes', 'Usenet', 'Technology Review', '®', 'Frequency analysis', 'OM Sterling Global University', 'Cipher', 'Parsing', 'Д', 'System operators', 'Online game', 'Binary numeral system', 'Instant messaging', 'Geek Code', 'Homoglyph', 'Elite', 'Orthography', 'Agent noun', 'Poetry', 'User (computing)', 'Padonkaffsky jargon', 'Wiktionary:über', 'Owned (slang)', 'Kevin Mitnick', 'Homestuck', 'Variety (linguistics)', 'Я', 'Ampersand', 'Евро', 'Warez', 'Cyberculture', '£', 'Chi (letter)', 'Bulletin board system', 'Stem (linguistics)', 'Palatal approximant', 'Escape_(Journey_album)', 'Online communities', 'Affricate']\n",
      "0 pages with no outgoing edge\n",
      "2297782 directed edges found\n",
      "11 classes level 0: [1126, 1408, 2422, 2979, 3148, 3310, 4255, 4290, 4681, 5318, 15575] (48512)\n",
      "32 classes level 1: [355, 360, 500, 608, 791, 849, 886, 886, 988, 1012, 1108, 1126, 1186, 1191, 1207, 1210, 1231, 1335, 1386, 1408, 1825, 1902, 2030, 2075, 2120, 2310, 2342, 2396, 2452, 2979, 3148, 3310] (48512)\n",
      "251 classes level 2: [1, 2, 3, 3, 3, 5, 5, 5, 6, 8, 8, 9, 10, 10, 10, 10, 10, 11, 12, 13, 14, 15, 15, 17, 17, 17, 17, 18, 18, 19, 20, 20, 20, 22, 23, 25, 25, 25, 29, 30, 30, 35, 35, 35, 35, 36, 36, 36, 37, 39, 39, 39, 40, 40, 40, 40, 40, 40, 41, 42, 42, 43, 43, 43, 44, 45, 45, 47, 47, 48, 48, 49, 50, 50, 51, 51, 51, 51, 52, 52, 52, 53, 54, 55, 56, 56, 57, 58, 59, 60, 60, 60, 60, 60, 61, 62, 63, 68, 69, 69, 69, 70, 71, 72, 72, 74, 74, 78, 78, 78, 79, 81, 84, 85, 85, 86, 88, 89, 89, 89, 89, 90, 91, 92, 93, 93, 94, 97, 104, 105, 109, 110, 110, 110, 111, 111, 114, 116, 117, 120, 121, 121, 122, 128, 129, 131, 132, 134, 136, 138, 139, 139, 140, 140, 147, 149, 151, 152, 160, 160, 167, 170, 172, 172, 175, 177, 185, 192, 194, 194, 196, 197, 198, 199, 200, 201, 201, 204, 219, 220, 221, 221, 226, 226, 230, 234, 237, 238, 241, 249, 250, 250, 267, 270, 271, 273, 275, 280, 295, 297, 298, 299, 300, 301, 302, 302, 303, 323, 338, 353, 357, 365, 369, 378, 382, 383, 387, 395, 411, 420, 440, 441, 451, 470, 490, 500, 501, 503, 518, 529, 539, 542, 546, 563, 567, 594, 629, 654, 668, 741, 803, 807, 825, 865, 873, 931, 989, 1146, 1424, 1503, 1725] (48512)\n",
      "Classes (level 2) with less than 3 nodes: ['People ->- Miscellaneous ->- Micronations', 'Physical sciences ->- Earth science ->- Earth science basics']\n",
      "Nodes in classes that have less than 3 nodes: ['31749258', '2890783', '20653168']\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "from collections import Counter\n",
    "import shutil\n",
    "\n",
    "# Let's use what we saved\n",
    "# \n",
    "abstracts_file = './outputs_save/__abstracts.txt'\n",
    "# abstracts_file = './outputs/__abstracts.txt'\n",
    "links_file = './outputs_save/__links.txt'\n",
    "headers_file = './outputs_save/__headers.txt'\n",
    "wikiid_title_file = './outputs_save/wikiid_title.txt'\n",
    "labels_file = './outputs_save/classification_unfiltered.txt'\n",
    "\n",
    "# 0) Let's filter the links (we'll keep only the links in the set of abstracts found)\n",
    "wikiid_title_dict = dict()\n",
    "with open(wikiid_title_file, 'r', encoding='utf8') as wikiid_title:\n",
    "    for l in wikiid_title:\n",
    "        wikiid, title = l.strip().split('\\t')[0], l.strip().split('\\t')[1]\n",
    "        wikiid_title_dict[wikiid] = title\n",
    "\n",
    "wikiid_title_dict_filtered = dict()\n",
    "with open(abstracts_file, 'r', encoding='utf8') as abstracts:\n",
    "    for l in abstracts:\n",
    "        try:\n",
    "            id = l.strip().split('\\t')[0]\n",
    "        except:\n",
    "            id = l.strip() \n",
    "        # if not id in ['31749258', '2890783', '20653168']: #Nodes that belongs to class with less than 3 elements (at level 2)\n",
    "        wikiid_title_dict_filtered[id] = wikiid_title_dict[id]\n",
    "print(f'{len(list(wikiid_title_dict_filtered.keys()))} articles')\n",
    "set_of_titles = set([i for i in wikiid_title_dict_filtered.values()])\n",
    "print(f'{len(list(wikiid_title_dict_filtered.keys()))-len(set_of_titles)} articles with the same name')\n",
    "\n",
    "# seen = set()\n",
    "# duplicates = [x.lower() for x in wikiid_title_dict_filtered.values() if x.lower() in seen or seen.add(x.lower())]\n",
    "# print(duplicates)\n",
    "    \n",
    "title_wikiid = dict()\n",
    "for k, v in wikiid_title_dict_filtered.items():\n",
    "    title_wikiid[v] = k  \n",
    "\n",
    "\n",
    "# 1) Let's check some infos\n",
    "with open(abstracts_file, 'r', encoding='utf8') as abstracts:\n",
    "    abstract_lengths = []\n",
    "    empty_abstracts = []\n",
    "    short_abstracts = [] # less than 50 chars\n",
    "    for l in abstracts:\n",
    "        try:\n",
    "            length = len(l.strip().split('\\t')[1])\n",
    "            abstract_lengths.append(length)\n",
    "            if length < 50:\n",
    "                short_abstracts.append(l.strip().split('\\t')[0])\n",
    "        except:\n",
    "            empty_abstracts.append(l.strip())\n",
    "            short_abstracts.append(l.strip())\n",
    "            abstract_lengths.append(0)\n",
    "    print(f'Abstracts:')\n",
    "    print(f'* Average abstract size: {statistics.mean(abstract_lengths)} ({statistics.stdev(abstract_lengths)})')\n",
    "    print(f'* {len(empty_abstracts)} empty abstracts')\n",
    "    print(f'* {len(short_abstracts)} ({100*len(short_abstracts)/len(abstract_lengths):.2f}%) short abstracts (less than 50 characters, can be considered empty):\\n  {[(wikiid_title_dict_filtered[v],v) for v in short_abstracts]}\\n')\n",
    "\n",
    "    for v in short_abstracts:\n",
    "        shutil.copy2(f'./outputs/wikivitals_raw_wikitexts/{v}.wt', './outputs/wikivitals_manual_changes/') # target filename is /dst/dir/file.ext\n",
    "\n",
    "# 2) Number of articles with no headers\n",
    "with open(headers_file, 'r', encoding='utf8') as headers:\n",
    "    no_header = []\n",
    "    for l in headers:\n",
    "        try:\n",
    "            id, headers_found = l.strip().split('\\t')\n",
    "            headers_found = [a.strip() for a in headers_found.split(' §§ ')]\n",
    "        except:\n",
    "            no_header.append(l.strip())\n",
    "    print(f'Headers:')\n",
    "    print(f'* {len(no_header)} articles with no header found:\\n  {[wikiid_title_dict_filtered[v] for v in no_header]}\\n')\n",
    "\n",
    "# 3) Get the links filtered\n",
    "outfile_links_filtered = open('./outputs/__links_filtered.txt', 'w', encoding = 'utf8')\n",
    "page_without_link = 0\n",
    "number_edges = 0\n",
    "\n",
    "# First, let's check the number of canonical name that starts with a lower letter \n",
    "tmp = [i for i in set_of_titles if i[0].islower()]\n",
    "print(f\"There are {len(tmp)} canonical names that startswith a lowercase letter\")\n",
    "with open(links_file, 'r', encoding = 'utf8') as links:\n",
    "    for l in links:\n",
    "        try:\n",
    "            id, links_found = l.strip().split('\\t')\n",
    "            if id == '18562':\n",
    "                print(links_found)\n",
    "                print('-------------')\n",
    "            links_found = [a.strip() for a in links_found.split(' §§ ')]\n",
    "            links_found = [a.replace(a[0], a[0].upper(), 1) for a in links_found]\n",
    "            if id == '18562':\n",
    "                print(links_found)\n",
    "            links_found_filtered = set_of_titles.intersection(set(links_found))\n",
    "            number_edges += len(links_found_filtered)\n",
    "            for wikiid in [title_wikiid[t] for t in links_found_filtered]:\n",
    "                outfile_links_filtered.write(f'{id}\\t{wikiid}\\t1.0\\n')\n",
    "        except:\n",
    "            id = l.strip() # case where there is no link in the page\n",
    "            links_found = []\n",
    "            page_without_link += 1\n",
    "            links_found_filtered = set()\n",
    "            outfile_links_filtered.write(f'{id}\\n')\n",
    "print(f'{page_without_link} pages with no outgoing edge')\n",
    "print(f'{number_edges} directed edges found')\n",
    "outfile_links_filtered.close()\n",
    "\n",
    "# 4) Calculate the classes\n",
    "outfile_classes_0 = open('./outputs/__classes0.txt', 'w', encoding = 'utf8')\n",
    "outfile_classes_1 = open('./outputs/__classes1.txt', 'w', encoding = 'utf8')\n",
    "outfile_classes_2 = open('./outputs/__classes2.txt', 'w', encoding = 'utf8')\n",
    "with open(labels_file, 'r', encoding='utf8') as labels:\n",
    "    set_class0, set_class1, set_class2 = [], [], []\n",
    "    ids = []\n",
    "    for l in labels:\n",
    "        try:\n",
    "            name, class0, class1, class2 = l.strip().split('\\t')\n",
    "            name = name.strip()\n",
    "            id = title_wikiid[name] # fails if id not in the keys\n",
    "            ids.append(id)\n",
    "            set_class0.append(class0)\n",
    "            set_class1.append(class0 + ' ->- ' + class1)\n",
    "            set_class2.append(class0 + ' ->- ' + class1 + ' ->- ' + class2)\n",
    "            outfile_classes_0.write(id + '\\t' + class0 + '\\n')\n",
    "            outfile_classes_1.write(id + '\\t' + class0 + ' ->- ' + class1 + '\\n')\n",
    "            outfile_classes_2.write(id + '\\t' + class0 + ' ->- ' + class1 + ' ->- ' + class2 + '\\n')\n",
    "        except:\n",
    "            None   \n",
    "outfile_classes_0.close()\n",
    "outfile_classes_1.close()\n",
    "outfile_classes_2.close()\n",
    "# print(set(ids).symmetric_difference(set(title_wikiid.values())))      \n",
    "print(f'{len(set(set_class0))} classes level 0: {sorted(Counter(set_class0).values())} ({sum(Counter(set_class0).values())})')\n",
    "print(f'{len(set(set_class1))} classes level 1: {sorted(Counter(set_class1).values())} ({sum(Counter(set_class1).values())})')\n",
    "print(f'{len(set(set_class2))} classes level 2: {sorted(Counter(set_class2).values())} ({sum(Counter(set_class2).values())})')   \n",
    "        \n",
    "tmp = Counter(set_class2)\n",
    "less_3 = [i for i in tmp.keys() if tmp[i]<3]\n",
    "print(f'Classes (level 2) with less than 3 nodes: {less_3}')\n",
    "indices_nodes_in_less_3 = [i for i in range(len(ids)) if set_class2[i] in less_3]\n",
    "nodes_in_less_3 = [ids[i] for i in indices_nodes_in_less_3]\n",
    "print(f'Nodes in classes that have less than 3 nodes: {nodes_in_less_3}')\n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll stem the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=0.001)\n",
    "\n",
    "tfidf_vectorizer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11939 features calculated for abstracts\n"
     ]
    }
   ],
   "source": [
    "abstracts_file = './outputs_save/__abstracts.txt'\n",
    "outfile_abstracts_vocabulary = open('./outputs/__abstracts_vocabulary.txt', 'w', encoding = 'utf8')\n",
    "outfile_abstracts_countvec = open('./outputs/__abstracts_countvec.txt', 'w', encoding = 'utf8')\n",
    "outfile_abstracts_tfidf = open('./outputs/__abstracts_tfidf.txt', 'w', encoding = 'utf8')\n",
    "\n",
    "with open(abstracts_file, 'r', encoding = 'utf8') as abstracts:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    stop_cnt = 0\n",
    "    for l in abstracts:\n",
    "        try:\n",
    "            id, abstract = l.strip().split('\\t')\n",
    "            abstract.replace('.', ' ')\n",
    "            corpus.append(abstract)\n",
    "        except:\n",
    "            id = l.strip()\n",
    "            corpus.append('')\n",
    "        ids.append(id)\n",
    "        stop_cnt += 1\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_tf = tfidf_vectorizer.fit_transform(X)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "nzX_rows, nzX_cols = np.nonzero(X)\n",
    "feats_dict = dict()\n",
    "for i in range(len(ids)):\n",
    "    feats_dict[i] = []\n",
    "for a, b in zip(nzX_rows, nzX_cols):\n",
    "    feats_dict[a].append((feature_names[b], X[a,b], X_tf[a,b]))\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'abs_{a.replace(\" \", \"_\")}:{b}' for a,b,_ in feats_dict[i]) + '\\n'\n",
    "    outfile_abstracts_countvec.write(str2write)\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'abs_{a.replace(\" \", \"_\")}:{b}' for a,_,b in feats_dict[i]) + '\\n'\n",
    "    outfile_abstracts_tfidf.write(str2write)\n",
    "outfile_abstracts_vocabulary.write('\\t'.join([f'abs_{a.replace(\" \", \"_\")}' for a in feature_names]))\n",
    "    \n",
    "\n",
    "\n",
    "outfile_abstracts_vocabulary.close()\n",
    "outfile_abstracts_countvec.close()\n",
    "outfile_abstracts_tfidf.close()\n",
    "\n",
    "print(f'{len(feature_names)} features calculated for abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 features calculated for headers\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "headers_file = './outputs_save/__headers.txt'\n",
    "outfile_headers_vocabulary = open('./outputs/__headers_vocabulary.txt', 'w', encoding = 'utf8')\n",
    "outfile_headers_countvec = open('./outputs/__headers_countvec.txt', 'w', encoding = 'utf8')\n",
    "outfile_headers_tfidf = open('./outputs/__headers_tfidf.txt', 'w', encoding = 'utf8')\n",
    "\n",
    "with open(headers_file, 'r', encoding = 'utf8') as headers:\n",
    "    corpus = []\n",
    "    global_corpus = []\n",
    "    ids = []\n",
    "    stop_cnt = 0\n",
    "    for l in headers:\n",
    "        try:\n",
    "            id, header = l.strip().split('\\t')\n",
    "            tmp = [re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",i) for i in header.split('§§')]\n",
    "            tmp = [' '.join(i.split()) for i in tmp]\n",
    "            global_corpus = global_corpus + tmp\n",
    "            # header = re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",header) # I keep numbers, letters (accentuated letters included)\n",
    "            header = ' '.join(tmp)\n",
    "            corpus.append(header)\n",
    "        except:\n",
    "            id = l.strip()\n",
    "            corpus.append('')\n",
    "        ids.append(id)\n",
    "        stop_cnt += 1\n",
    "\n",
    "# print(corpus)\n",
    "vectorizer.fit(global_corpus)\n",
    "# vocab = vectorizer.vocabulary_\n",
    "X = vectorizer.transform(corpus)\n",
    "X_tf = tfidf_vectorizer.fit_transform(X)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "nzX_rows, nzX_cols = np.nonzero(X)\n",
    "feats_dict = dict()\n",
    "for i in range(len(ids)):\n",
    "    feats_dict[i] = []\n",
    "for a, b in zip(nzX_rows, nzX_cols):\n",
    "    feats_dict[a].append((feature_names[b], X[a,b], X_tf[a,b]))\n",
    "for i in range(len(ids)):\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'hea_{a.replace(\" \", \"_\")}:{b}' for a,b,_ in feats_dict[i]) + '\\n'\n",
    "    outfile_headers_countvec.write(str2write)\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'hea_{a.replace(\" \", \"_\")}:{b}' for a,_,b in feats_dict[i]) + '\\n'\n",
    "    outfile_headers_tfidf.write(str2write)\n",
    "outfile_headers_vocabulary.write('\\t'.join([f'hea_{a.replace(\" \", \"_\")}' for a in feature_names]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "outfile_headers_vocabulary.close()\n",
    "outfile_headers_countvec.close()\n",
    "outfile_headers_tfidf.close()\n",
    "\n",
    "print(f'{len(feature_names)} features calculated for headers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664 features calculated for titles\n"
     ]
    }
   ],
   "source": [
    "# abstracts_file = './outputs_save/__abstracts.txt'\n",
    "wikiid_title_file = './outputs_save/wikiid_title.txt'\n",
    "\n",
    "outfile_titles_vocabulary = open('./outputs/__titles_vocabulary.txt', 'w', encoding = 'utf8')\n",
    "outfile_titles_countvec = open('./outputs/__titles_countvec.txt', 'w', encoding = 'utf8')\n",
    "outfile_titles_tfidf = open('./outputs/__titles_tfidf.txt', 'w', encoding = 'utf8')\n",
    "outfile_titles = open('./outputs/__titles.txt', 'w', encoding = 'utf8')\n",
    "\n",
    "# WARNING: min_df changed for titles\n",
    "vectorizer = StemmedCountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=0.0001)\n",
    "\n",
    "# 0) Let's filter the titles (we'll keep only the links in the set of abstracts found)\n",
    "wikiid_title_dict = dict()\n",
    "with open(wikiid_title_file, 'r', encoding='utf8') as wikiid_title:\n",
    "    for l in wikiid_title:\n",
    "        wikiid, title = l.strip().split('\\t')\n",
    "        wikiid_title_dict[wikiid] = title\n",
    "\n",
    "wikiid_title_dict_filtered = dict()\n",
    "with open(abstracts_file, 'r', encoding='utf8') as abstracts:\n",
    "    ids = []\n",
    "    for l in abstracts:\n",
    "        try:\n",
    "            id = l.strip().split('\\t')[0]\n",
    "            ids.append(id)\n",
    "        except:\n",
    "            id = l.strip() \n",
    "            ids.append(id)\n",
    "        wikiid_title_dict_filtered[id] = wikiid_title_dict[id]\n",
    "\n",
    "corpus = [wikiid_title_dict_filtered[id] for id in ids]\n",
    "corpus = [re.sub(r\"[^a-zA-Z0-9\\u00C0-\\u00FF.\\s]\",\" \",i) for i in corpus]\n",
    "corpus = [' '.join(i.split()) for i in corpus]\n",
    "for id in ids:\n",
    "    outfile_titles.write(f'{id}\\t{wikiid_title_dict_filtered[id]}\\n')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_tf = tfidf_vectorizer.fit_transform(X)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "nzX_rows, nzX_cols = np.nonzero(X)\n",
    "feats_dict = dict()\n",
    "for i in range(len(ids)):\n",
    "    feats_dict[i] = []\n",
    "for a, b in zip(nzX_rows, nzX_cols):\n",
    "    feats_dict[a].append((feature_names[b], X[a,b], X_tf[a,b]))\n",
    "for i in range(len(ids)):\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'tit_{a.replace(\" \", \"_\")}:{b}' for a,b,_ in feats_dict[i]) + '\\n'\n",
    "    outfile_titles_countvec.write(str2write)\n",
    "    str2write = ids[i] + '\\t' + ' '.join(f'tit_{a.replace(\" \", \"_\")}:{b}' for a,_,b in feats_dict[i]) + '\\n'\n",
    "    outfile_titles_tfidf.write(str2write)\n",
    "outfile_titles_vocabulary.write('\\t'.join([f'tit_{a.replace(\" \", \"_\")}' for a in feature_names]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "outfile_titles_vocabulary.close()\n",
    "outfile_titles_countvec.close()\n",
    "outfile_titles_tfidf.close()\n",
    "outfile_titles.close()\n",
    "\n",
    "print(f'{len(feature_names)} features calculated for titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHI-2 selection of features\n",
    "# Let's select 4000 most 'predictive' features for class level 3 \n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "def get_stems(list_of_vocabulary_files):\n",
    "    stems_indices = dict()\n",
    "    i = 0\n",
    "    for file in list_of_vocabulary_files:\n",
    "        with open(f'./outputs_save/{file}', 'r', encoding = 'utf8') as v_f:\n",
    "            stems_full = v_f.readline().split('\\t')\n",
    "            for s in stems_full:\n",
    "                stems_indices[s] = i\n",
    "                i += 1\n",
    "    return(stems_indices)\n",
    "\n",
    "def get_article_indices(id_whatever_file):\n",
    "    wikiids_indices = dict()\n",
    "    i = 0\n",
    "    with open(f'./outputs_save/{id_whatever_file}', 'r', encoding = 'utf8') as id_whatever:\n",
    "        for line in id_whatever:\n",
    "            id = line.strip().split('\\t')[0]\n",
    "            wikiids_indices[id] = i\n",
    "            i+=1\n",
    "    return wikiids_indices\n",
    "\n",
    "def load_features(list_of_feature_files, list_of_vocabulary_files):\n",
    "    stems_indices = get_stems(list_of_vocabulary_files)\n",
    "    wikiids_indices = get_article_indices(list_of_feature_files[0])\n",
    "    rows_, cols_, data_ = [], [], []\n",
    "    for file in list_of_feature_files:\n",
    "        with open(f'./outputs_save/{file}', 'r', encoding = 'utf8') as id_feat:\n",
    "            for line in id_feat:\n",
    "                try:\n",
    "                    id = wikiids_indices[line.strip().split('\\t')[0]] \n",
    "                except:\n",
    "                    break\n",
    "                try: \n",
    "                    features_str = line.strip().split('\\t')[1]\n",
    "                except:\n",
    "                    features_str = ''\n",
    "                cols = [stems_indices[stem_cnt.split(':')[0]] for stem_cnt in features_str.strip().split()]\n",
    "                cols_ += cols\n",
    "                rows_ += [id for _ in range(len(cols))]\n",
    "                data_ += [1 for _ in range(len(cols))]\n",
    "    num_nodes = len(wikiids_indices.keys())\n",
    "    vocab_size = len(stems_indices.keys())\n",
    "    tmp = sorted([(v,k) for k,v in stems_indices.items()])\n",
    "    ordered_stems = [k for (_, k) in tmp]\n",
    "    return coo_matrix((data_, (rows_, cols_)), shape=(num_nodes, vocab_size)), ordered_stems\n",
    "\n",
    "def load_target(target_file, ref_file):\n",
    "    wikiids_indices = get_article_indices(ref_file)\n",
    "    targets = dict()\n",
    "    y_out = [[None] for _ in range(len(wikiids_indices.keys()))]\n",
    "    with open(f'./outputs_save/{target_file}', 'r', encoding = 'utf8') as id_target:\n",
    "        for line in id_target:\n",
    "            if not line.strip() == '':\n",
    "                wikiid, target_str = line.strip().split('\\t')\n",
    "                id = wikiids_indices[wikiid] \n",
    "                targets[id] = target_str\n",
    "    for id in targets.keys():\n",
    "        y_out[id][0] = targets[id]\n",
    "    return np.array(y_out)\n",
    "    \n",
    "\n",
    "\n",
    "X, feats = load_features([\n",
    "        '__abstracts_countvec.txt',\n",
    "        '__headers_countvec.txt' ,\n",
    "        '__titles_countvec.txt'\n",
    "    ], \n",
    "    [\n",
    "        '__abstracts_vocabulary.txt',\n",
    "        '__headers_vocabulary.txt',\n",
    "        '__titles_vocabulary.txt'\n",
    "    ])  \n",
    "\n",
    "# X, feats = load_features([\n",
    "#         '__headers_countvec.txt' ,\n",
    "#         '__titles_countvec.txt'\n",
    "#     ], \n",
    "#     [\n",
    "#         '__headers_vocabulary.txt',\n",
    "#         '__titles_vocabulary.txt'\n",
    "#     ])    \n",
    "\n",
    "y_target = load_target(\n",
    "    '__classes2.txt',\n",
    "    '__abstracts_countvec.txt'\n",
    ")\n",
    "\n",
    "# for i in y_target:\n",
    "#     if i == [None]:\n",
    "#         print(i)\n",
    "\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=4000)\n",
    "X_new = ch2.fit_transform(X, y_target)\n",
    "feats1 = ch2.get_feature_names_out(input_features=feats)\n",
    "\n",
    "\n",
    "\n",
    "y_target = load_target(\n",
    "    '__classes1.txt',\n",
    "    '__abstracts_countvec.txt'\n",
    ")\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=4000)\n",
    "ch2.fit(X, y_target)\n",
    "feats2 = ch2.get_feature_names_out(input_features=feats)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered vocabulary and the definitive features\n",
    "feats_f = open('./outputs/__wikivitals_features_one_hot.txt', 'w', encoding='utf8')\n",
    "vocab_f = open('./outputs/__wikivitals_vocabulary.txt', 'w', encoding='utf8')\n",
    "\n",
    "# 1- Save the definitive vocabulary\n",
    "vocab_f.write('\\t'.join(feats1))\n",
    "vocab_f.close()\n",
    "\n",
    "# 2- Save the features (one-hot representations)\n",
    "X_new_ = X_new.tocoo()\n",
    "wikiid_indices = get_article_indices('__abstracts_countvec.txt') # wiki id : id (from 0 to (num. articles - 1))\n",
    "sorted_wikiids = [k for v,k in sorted([(v_,k_) for k_, v_ in wikiid_indices.items()])]\n",
    "wikiid_feats_couples = zip(\n",
    "    [sorted_wikiids[i] for i in X_new_.row], \n",
    "    [feats1[i] for i in X_new_.col])\n",
    "id_feats = dict()\n",
    "for wikiid, feat in wikiid_feats_couples:\n",
    "    id_feats[wikiid] = id_feats.get(wikiid, [])\n",
    "    id_feats[wikiid].append(feat)\n",
    "\n",
    "for k,v in id_feats.items():\n",
    "    l = str(k) + '\\t' + ' '.join([str(j)+':1' for j in v]) + '\\n'\n",
    "    feats_f.write(l)\n",
    "\n",
    "feats_f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of features per articles (with standard deviation):\n",
      "58.47 (34.61)\n",
      "Number of features per source:\n",
      "Counter({'abs': 3452, 'tit': 370, 'hea': 178})\n"
     ]
    }
   ],
   "source": [
    "# Display some statistics\n",
    "import statistics\n",
    "\n",
    "print('Average number of features per articles (with standard deviation):')\n",
    "lengths = [len(v) for v in id_feats.values()]\n",
    "m, std = statistics.mean(lengths), statistics.stdev(lengths)\n",
    "print(f'{m:.2f} ({std:.2f})')\n",
    "\n",
    "print('Number of features per source:')\n",
    "prefixes = Counter([i[:3] for i in feats1])\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the templates (links at the end of an article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles1.xml-p1p41242.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles10.xml-p4045403p5399366.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p5399367p6899366.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles11.xml-p6899367p7054859.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p7054860p8554859.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles12.xml-p8554860p9172788.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p10672789p11659682.bz2\n",
      "1000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles13.xml-p9172789p10672788.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p11659683p13159682.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles14.xml-p13159683p14324602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p14324603p15824602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p15824603p17324602.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles15.xml-p17324603p17460152.bz2\n",
      "2000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p17460153p18960152.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p18960153p20460152.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles16.xml-p20460153p20570392.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p20570393p22070392.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p22070393p23570392.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles17.xml-p23570393p23716197.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p23716198p25216197.bz2\n",
      "3000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p25216198p26716197.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles18.xml-p26716198p27121850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p27121851p28621850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p28621851p30121850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles19.xml-p30121851p31308442.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles2.xml-p41243p151573.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p31308443p32808442.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p32808443p34308442.bz2\n",
      "4000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles20.xml-p34308443p35522432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p35522433p37022432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p37022433p38522432.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles21.xml-p38522433p39996245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p39996246p41496245.bz2\n",
      "5000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p41496246p42996245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p42996246p44496245.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles22.xml-p44496246p44788941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p44788942p46288941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p46288942p47788941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p47788942p49288941.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles23.xml-p49288942p50564553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p50564554p52064553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p52064554p53564553.bz2\n",
      "6000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p53564554p55064553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p55064554p56564553.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles24.xml-p56564554p57025655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p57025656p58525655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p58525656p60025655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p60025656p61525655.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles25.xml-p61525656p62585850.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles26.xml-p62585851p63975909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p63975910p65475909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p65475910p66975909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p66975910p68475909.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p68475910p69975909.bz2\n",
      "7000 templates found\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles27.xml-p69975910p70448383.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles3.xml-p151574p311329.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles4.xml-p311330p558391.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles5.xml-p558392p958045.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles6.xml-p958046p1483661.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles7.xml-p1483662p2134111.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles8.xml-p2134112p2936260.bz2\n",
      "C:/Users/Antoine/Downloads/enwiki-split/enwiki-20220401-pages-articles9.xml-p2936261p4045402.bz2\n",
      "7903\n"
     ]
    }
   ],
   "source": [
    "# import mwxml\n",
    "# import mwtypes\n",
    "# import wikitextparser\n",
    "# # Note: also install wikipedia_sections (pip install wikipedia_sections)\n",
    "\n",
    "# import re\n",
    "\n",
    "# # Internal path to the Wikipedia dump split\n",
    "# wikidump_splits_path = \"C:/Users/Antoine/Downloads/enwiki-split\"\n",
    "# dump_files = os.listdir(wikidump_splits_path)\n",
    "# files = [wikidump_splits_path + '/' + f for f in dump_files]\n",
    "# files = files\n",
    "\n",
    "\n",
    "\n",
    "# # Set of all article canonical titles\n",
    "# article_titles = set(list(articles_classification.keys()))\n",
    "\n",
    "\n",
    "\n",
    "# def page_info(dump, path):\n",
    "#     \"\"\"\n",
    "#     Read the dump page per page. Yield infos (id, title, redirect, and text)\n",
    "#     only for template pages in the set of wikivitals articles (global var)\n",
    "#     (can be paralellized, can't make it work)\n",
    "\n",
    "#     Input: \n",
    "#     * dump: list of Wikidumps (with format .xml.bz2)\n",
    "#     * path: str - path to Wikidump split   \n",
    "#     \"\"\"\n",
    "#     for page in dump:\n",
    "#         tstamp = mwtypes.Timestamp(0)\n",
    "#         if page.title in article_titles and page.redirect == None and page.namespace == 10:\n",
    "#             for revision in page:\n",
    "#                 if revision.timestamp > tstamp:\n",
    "#                     last_revision = revision\n",
    "#                     tstamp = revision.timestamp\n",
    "#                 text = last_revision.text\n",
    "#             yield page.id, page.title, page.redirect, text\n",
    "\n",
    "\n",
    "# # If some articles are redirections, we'll exclude them from our set of articles\n",
    "# redirections = []\n",
    "# raw_abstract_kept = []\n",
    "# templates_content = dict()\n",
    "# counter = 0\n",
    "\n",
    "# # adjustment1 = re.compile(r'thumb\\|.*?\\n')\n",
    "# adjustment2 = re.compile(r'\\{\\|.*\\|\\}', re.DOTALL)\n",
    "# adjustment3 = re.compile(r'\\[\\[.*\\]\\]', re.DOTALL)\n",
    "# # adjustment4 = re.compile(r'^.*\\|.*?\\n', re.DOTALL)\n",
    "\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "#     for id, title, redirect, wikitext in mwxml.map(page_info, [file]):\n",
    "        \n",
    "#         # Exclude redirections \n",
    "#         if not redirect == None:\n",
    "#             redirections.append(title)\n",
    "#         else:\n",
    "\n",
    "#             # Find the internal links\n",
    "#             links, _ = wikilinks_namespace0(wikitext)\n",
    "            \n",
    "  \n",
    "\n",
    "#             # if (not title in articles_content.keys()) or (articles_content[title]['clean abstract'] == None):\n",
    "#             templates_content[title] = {\n",
    "#                 'id': id,\n",
    "#                 'title': title,\n",
    "#                 'links': set(links),\n",
    "#                 'wikitext': wikitext.replace('\\n', ' ')\n",
    "#             }\n",
    "#             counter +=1\n",
    "#             if counter%1000 == 0:\n",
    "#                 print(f'{counter} templates found')\n",
    "\n",
    "# print(len(list(templates_content.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directed graph: 2297782 nodes\n",
      "Undirected graph: 4132534 nodes\n",
      "\n",
      "Edge homophily for class level 0: 0.34\n",
      "Edge homophily for class level 1: 0.24\n",
      "Edge homophily for class level 2: 0.15\n",
      "\n",
      "Average outgoing degree: 85.19 (141.23)\n",
      "Max & min degree: 7720 & 0\n",
      "Number of isolated nodes: 16\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "links_file = './outputs_save/__links_filtered.txt'\n",
    "class0_file = './outputs_save/__classes0.txt'\n",
    "class1_file = './outputs_save/__classes1.txt'\n",
    "class2_file = './outputs_save/__classes2.txt'\n",
    "\n",
    "def file2dict(file):\n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    d = dict()\n",
    "    for l in f:\n",
    "        i, j = l.strip().split('\\t')\n",
    "        d[i] = j\n",
    "    return d\n",
    "\n",
    "with open(links_file, 'r', encoding='utf8') as links:\n",
    "    set_of_links = set()\n",
    "    cnt = 0\n",
    "    for l in links:\n",
    "        try:\n",
    "            start, end, w = l.strip().split('\\t')\n",
    "            set_of_links.add((start, end))\n",
    "            set_of_links.add((end, start))\n",
    "            cnt += 1\n",
    "        except:\n",
    "            None\n",
    "print(f'Directed graph: {cnt} nodes')\n",
    "num_edges_undirected = len(set_of_links)\n",
    "print(f'Undirected graph: {num_edges_undirected} nodes\\n')\n",
    "\n",
    "\n",
    "# Homophily metrics (in undirected graph)\n",
    "class0_ = file2dict(class0_file)\n",
    "class1_ = file2dict(class1_file)\n",
    "class2_ = file2dict(class2_file)\n",
    "H0 = sum([1 for a,b in set_of_links if class0_[a] == class0_[b]]) / num_edges_undirected\n",
    "H1 = sum([1 for a,b in set_of_links if class1_[a] == class1_[b]]) / num_edges_undirected\n",
    "H2 = sum([1 for a,b in set_of_links if class2_[a] == class2_[b]]) / num_edges_undirected\n",
    "print(f'Edge homophily for class level 0: {H0:.2f}')\n",
    "print(f'Edge homophily for class level 1: {H1:.2f}')\n",
    "print(f'Edge homophily for class level 2: {H2:.2f}\\n')\n",
    "\n",
    "# Edge distribution\n",
    "nodes_id = [k for k in class0_.keys()]\n",
    "degrees = dict()\n",
    "for id in nodes_id:\n",
    "    degrees[id] = 0\n",
    "for a,b in set_of_links:\n",
    "    degrees[a] += 1\n",
    "degrees_list = [v for v in degrees.values()]\n",
    "print(f'Average outgoing degree: {mean(degrees_list):.2f} ({stdev(degrees_list):.2f})')\n",
    "print(f'Max & min degree: {max(degrees_list)} & {min(degrees_list)}')\n",
    "print(f'Number of isolated nodes: {len([i for i in degrees_list if i == 0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum degree: 7720\n",
      "Max node count with the same degree: 644\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "type": "scatter",
         "x": [
          267,
          388,
          83,
          75,
          20,
          466,
          40,
          68,
          115,
          9,
          104,
          44,
          118,
          77,
          326,
          19,
          131,
          10,
          27,
          21,
          18,
          16,
          62,
          139,
          37,
          145,
          80,
          199,
          85,
          63,
          111,
          51,
          67,
          23,
          97,
          30,
          82,
          127,
          28,
          96,
          135,
          76,
          88,
          176,
          99,
          31,
          201,
          36,
          73,
          32,
          426,
          14,
          81,
          233,
          48,
          166,
          220,
          47,
          259,
          266,
          113,
          128,
          252,
          347,
          130,
          290,
          132,
          56,
          64,
          42,
          71,
          29,
          69,
          100,
          33,
          66,
          79,
          150,
          43,
          60,
          74,
          116,
          70,
          214,
          22,
          182,
          287,
          39,
          119,
          822,
          180,
          114,
          38,
          59,
          129,
          103,
          134,
          7,
          84,
          89,
          140,
          49,
          173,
          275,
          25,
          11,
          147,
          26,
          46,
          45,
          120,
          41,
          102,
          142,
          86,
          227,
          15,
          53,
          136,
          50,
          172,
          72,
          109,
          126,
          8,
          245,
          196,
          55,
          159,
          164,
          238,
          390,
          215,
          24,
          78,
          54,
          35,
          158,
          94,
          273,
          101,
          122,
          210,
          149,
          6,
          447,
          231,
          65,
          816,
          727,
          170,
          391,
          254,
          581,
          186,
          98,
          168,
          5,
          12,
          121,
          34,
          4,
          87,
          241,
          896,
          107,
          190,
          61,
          440,
          520,
          124,
          468,
          261,
          171,
          108,
          146,
          299,
          144,
          268,
          282,
          174,
          429,
          288,
          488,
          95,
          179,
          300,
          216,
          112,
          345,
          138,
          511,
          110,
          105,
          123,
          161,
          260,
          316,
          654,
          143,
          117,
          239,
          57,
          249,
          545,
          305,
          244,
          247,
          148,
          184,
          198,
          157,
          197,
          52,
          208,
          92,
          58,
          229,
          1414,
          193,
          137,
          91,
          13,
          17,
          90,
          524,
          661,
          219,
          376,
          125,
          181,
          194,
          167,
          3,
          141,
          189,
          2,
          155,
          106,
          204,
          322,
          369,
          169,
          162,
          151,
          185,
          537,
          206,
          257,
          163,
          262,
          133,
          177,
          191,
          207,
          221,
          183,
          178,
          93,
          336,
          226,
          160,
          230,
          240,
          533,
          538,
          218,
          271,
          358,
          255,
          554,
          253,
          442,
          200,
          286,
          362,
          153,
          461,
          543,
          503,
          778,
          668,
          211,
          352,
          361,
          380,
          313,
          165,
          339,
          246,
          232,
          333,
          228,
          235,
          265,
          212,
          256,
          356,
          192,
          0,
          224,
          195,
          377,
          152,
          301,
          320,
          251,
          772,
          281,
          279,
          175,
          243,
          549,
          325,
          371,
          366,
          223,
          650,
          769,
          321,
          222,
          154,
          156,
          550,
          602,
          285,
          1,
          415,
          399,
          217,
          341,
          296,
          264,
          525,
          437,
          975,
          363,
          410,
          386,
          460,
          843,
          304,
          328,
          331,
          311,
          202,
          270,
          364,
          277,
          307,
          465,
          416,
          242,
          318,
          421,
          409,
          327,
          187,
          815,
          334,
          765,
          209,
          203,
          284,
          454,
          205,
          403,
          276,
          483,
          732,
          348,
          263,
          225,
          611,
          298,
          278,
          375,
          502,
          374,
          1120,
          760,
          315,
          821,
          1179,
          1108,
          370,
          408,
          445,
          385,
          295,
          444,
          392,
          302,
          379,
          283,
          297,
          236,
          323,
          293,
          317,
          504,
          389,
          340,
          188,
          309,
          484,
          528,
          367,
          413,
          749,
          248,
          912,
          430,
          527,
          500,
          384,
          506,
          292,
          552,
          434,
          401,
          508,
          432,
          308,
          360,
          642,
          368,
          306,
          330,
          280,
          344,
          237,
          419,
          411,
          329,
          495,
          646,
          213,
          314,
          824,
          372,
          570,
          250,
          324,
          234,
          338,
          359,
          294,
          354,
          436,
          700,
          600,
          536,
          291,
          433,
          709,
          472,
          455,
          678,
          349,
          493,
          1267,
          664,
          521,
          467,
          422,
          400,
          499,
          514,
          381,
          353,
          303,
          428,
          424,
          449,
          457,
          319,
          343,
          758,
          607,
          710,
          423,
          335,
          777,
          854,
          751,
          556,
          1137,
          3837,
          387,
          649,
          1079,
          1152,
          498,
          1584,
          453,
          476,
          682,
          1412,
          289,
          544,
          438,
          1557,
          420,
          395,
          963,
          597,
          458,
          720,
          258,
          532,
          350,
          1021,
          365,
          713,
          509,
          999,
          383,
          892,
          531,
          404,
          480,
          482,
          337,
          619,
          351,
          459,
          448,
          620,
          452,
          903,
          830,
          689,
          871,
          624,
          405,
          639,
          346,
          1344,
          924,
          332,
          272,
          510,
          474,
          1140,
          439,
          803,
          407,
          2479,
          393,
          1411,
          497,
          844,
          582,
          743,
          517,
          2976,
          565,
          481,
          764,
          890,
          656,
          1511,
          450,
          857,
          534,
          676,
          809,
          425,
          310,
          728,
          274,
          798,
          2095,
          1670,
          5402,
          342,
          470,
          1373,
          601,
          1646,
          1009,
          879,
          355,
          898,
          726,
          891,
          518,
          382,
          805,
          652,
          560,
          402,
          571,
          492,
          578,
          813,
          406,
          397,
          645,
          1779,
          540,
          911,
          606,
          1417,
          3172,
          2439,
          573,
          673,
          1134,
          626,
          1493,
          1045,
          706,
          1533,
          1105,
          1093,
          1187,
          622,
          589,
          451,
          690,
          641,
          1776,
          1076,
          1431,
          667,
          859,
          1692,
          566,
          1092,
          908,
          3266,
          1316,
          948,
          687,
          357,
          491,
          515,
          490,
          779,
          583,
          763,
          1001,
          1544,
          915,
          2504,
          1163,
          1131,
          1158,
          3489,
          2661,
          418,
          1877,
          1385,
          3336,
          608,
          1309,
          638,
          431,
          688,
          1097,
          1440,
          1313,
          2530,
          1365,
          2034,
          2377,
          1456,
          1195,
          496,
          631,
          269,
          553,
          832,
          701,
          615,
          1077,
          754,
          568,
          417,
          462,
          398,
          394,
          599,
          588,
          575,
          526,
          412,
          373,
          539,
          2171,
          312,
          1202,
          489,
          721,
          880,
          591,
          1300,
          839,
          610,
          923,
          595,
          733,
          519,
          435,
          1148,
          739,
          378,
          557,
          542,
          2038,
          598,
          592,
          551,
          427,
          555,
          695,
          651,
          479,
          1037,
          1389,
          2592,
          1241,
          719,
          846,
          1585,
          1225,
          477,
          964,
          530,
          894,
          717,
          471,
          819,
          980,
          494,
          513,
          976,
          562,
          653,
          485,
          507,
          788,
          662,
          396,
          759,
          628,
          766,
          546,
          1262,
          1506,
          1648,
          840,
          579,
          604,
          1023,
          1889,
          860,
          463,
          1094,
          1653,
          698,
          627,
          1136,
          1211,
          847,
          1006,
          614,
          535,
          475,
          621,
          699,
          940,
          666,
          1306,
          680,
          1347,
          1198,
          3069,
          5607,
          814,
          469,
          1348,
          2892,
          633,
          1054,
          1404,
          703,
          1277,
          1141,
          873,
          473,
          618,
          443,
          723,
          990,
          738,
          736,
          523,
          776,
          1025,
          1292,
          962,
          685,
          637,
          715,
          636,
          572,
          684,
          674,
          1015,
          446,
          858,
          512,
          2266,
          2177,
          986,
          623,
          2005,
          836,
          1002,
          505,
          1208,
          1594,
          441,
          864,
          954,
          1144,
          522,
          629,
          541,
          767,
          848,
          971,
          529,
          741,
          1004,
          889,
          686,
          961,
          559,
          895,
          956,
          694,
          585,
          647,
          612,
          2051,
          486,
          1214,
          564,
          558,
          487,
          456,
          501,
          925,
          516,
          1244,
          576,
          675,
          978,
          1240,
          660,
          781,
          714,
          795,
          901,
          922,
          1526,
          1342,
          1032,
          1434,
          711,
          2750,
          635,
          1760,
          640,
          1000,
          913,
          966,
          1035,
          1285,
          1252,
          756,
          1224,
          2769,
          594,
          1426,
          1923,
          1437,
          989,
          2697,
          7720,
          1520,
          1226,
          2753,
          691,
          1919,
          4345,
          648,
          1028,
          1031,
          1744,
          804,
          586,
          983,
          2735,
          630,
          625,
          643,
          867,
          665,
          616,
          1626,
          1743,
          1713,
          1065,
          634,
          672,
          547,
          853,
          2139,
          577,
          863,
          1253
         ],
         "y": [
          21,
          9,
          186,
          240,
          606,
          3,
          455,
          288,
          119,
          497,
          157,
          429,
          103,
          250,
          10,
          604,
          97,
          490,
          600,
          596,
          607,
          611,
          308,
          85,
          469,
          77,
          225,
          38,
          207,
          338,
          143,
          397,
          254,
          593,
          159,
          525,
          182,
          116,
          644,
          151,
          96,
          267,
          189,
          56,
          163,
          537,
          34,
          537,
          267,
          545,
          5,
          570,
          224,
          37,
          402,
          57,
          28,
          412,
          23,
          17,
          128,
          105,
          14,
          9,
          117,
          11,
          105,
          381,
          299,
          435,
          267,
          503,
          267,
          162,
          534,
          323,
          228,
          67,
          452,
          345,
          246,
          129,
          277,
          26,
          594,
          39,
          12,
          455,
          111,
          1,
          50,
          129,
          437,
          332,
          113,
          144,
          96,
          424,
          189,
          182,
          78,
          415,
          51,
          15,
          627,
          542,
          70,
          529,
          408,
          451,
          106,
          473,
          151,
          95,
          214,
          33,
          609,
          365,
          66,
          404,
          44,
          258,
          131,
          91,
          427,
          26,
          23,
          370,
          63,
          49,
          27,
          8,
          28,
          623,
          233,
          372,
          494,
          70,
          167,
          16,
          172,
          131,
          38,
          63,
          368,
          7,
          25,
          297,
          1,
          1,
          56,
          7,
          23,
          3,
          40,
          152,
          51,
          308,
          587,
          115,
          519,
          254,
          187,
          23,
          1,
          148,
          49,
          295,
          4,
          1,
          107,
          3,
          14,
          54,
          134,
          81,
          10,
          76,
          18,
          15,
          44,
          4,
          10,
          2,
          185,
          43,
          14,
          27,
          134,
          7,
          101,
          4,
          133,
          143,
          132,
          49,
          13,
          8,
          1,
          84,
          105,
          13,
          328,
          22,
          2,
          20,
          19,
          29,
          65,
          49,
          31,
          62,
          43,
          389,
          30,
          178,
          330,
          19,
          1,
          36,
          81,
          185,
          612,
          612,
          170,
          2,
          6,
          40,
          10,
          86,
          51,
          34,
          64,
          187,
          73,
          40,
          113,
          62,
          119,
          31,
          11,
          8,
          55,
          59,
          64,
          43,
          4,
          32,
          13,
          60,
          19,
          84,
          28,
          49,
          31,
          16,
          52,
          45,
          182,
          12,
          24,
          63,
          26,
          19,
          3,
          6,
          25,
          10,
          7,
          17,
          1,
          7,
          7,
          35,
          8,
          4,
          57,
          7,
          3,
          4,
          1,
          2,
          30,
          10,
          5,
          7,
          15,
          50,
          9,
          17,
          27,
          6,
          24,
          19,
          17,
          41,
          18,
          11,
          48,
          16,
          34,
          41,
          13,
          61,
          8,
          10,
          17,
          2,
          12,
          13,
          40,
          25,
          4,
          13,
          7,
          13,
          36,
          2,
          3,
          11,
          19,
          57,
          75,
          1,
          4,
          13,
          49,
          4,
          2,
          33,
          9,
          9,
          12,
          4,
          1,
          2,
          7,
          4,
          7,
          4,
          2,
          16,
          4,
          10,
          12,
          37,
          17,
          8,
          17,
          14,
          4,
          5,
          26,
          8,
          4,
          6,
          9,
          52,
          2,
          10,
          3,
          26,
          22,
          12,
          3,
          32,
          2,
          15,
          3,
          2,
          7,
          23,
          22,
          2,
          9,
          8,
          8,
          3,
          4,
          1,
          2,
          3,
          1,
          1,
          1,
          9,
          4,
          7,
          5,
          12,
          7,
          5,
          8,
          9,
          14,
          10,
          16,
          9,
          13,
          5,
          3,
          9,
          11,
          31,
          13,
          4,
          2,
          7,
          7,
          3,
          10,
          1,
          3,
          6,
          5,
          6,
          3,
          10,
          2,
          5,
          6,
          5,
          10,
          8,
          14,
          2,
          7,
          13,
          6,
          18,
          7,
          19,
          3,
          6,
          10,
          2,
          1,
          27,
          8,
          2,
          4,
          6,
          10,
          5,
          28,
          7,
          7,
          13,
          8,
          10,
          4,
          2,
          2,
          7,
          3,
          1,
          5,
          4,
          2,
          5,
          3,
          1,
          2,
          7,
          2,
          4,
          4,
          1,
          3,
          3,
          6,
          9,
          3,
          5,
          5,
          1,
          14,
          7,
          1,
          2,
          2,
          6,
          6,
          3,
          1,
          2,
          3,
          2,
          1,
          7,
          2,
          1,
          1,
          3,
          1,
          2,
          2,
          2,
          1,
          12,
          5,
          5,
          1,
          5,
          7,
          2,
          3,
          3,
          3,
          13,
          1,
          6,
          2,
          7,
          3,
          3,
          1,
          6,
          1,
          3,
          5,
          2,
          4,
          5,
          3,
          5,
          5,
          1,
          4,
          5,
          1,
          1,
          2,
          1,
          1,
          4,
          4,
          6,
          1,
          1,
          11,
          10,
          5,
          2,
          1,
          5,
          3,
          3,
          1,
          3,
          1,
          2,
          1,
          3,
          2,
          4,
          1,
          2,
          4,
          1,
          1,
          1,
          1,
          2,
          1,
          3,
          1,
          3,
          4,
          11,
          2,
          14,
          1,
          1,
          1,
          1,
          5,
          5,
          1,
          3,
          1,
          2,
          1,
          8,
          1,
          1,
          1,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          1,
          3,
          4,
          2,
          1,
          3,
          1,
          3,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          2,
          2,
          4,
          3,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          2,
          5,
          2,
          2,
          4,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          2,
          1,
          1,
          6,
          1,
          2,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          1,
          2,
          2,
          4,
          2,
          1,
          2,
          1,
          1,
          1,
          2,
          5,
          3,
          6,
          5,
          1,
          1,
          1,
          1,
          4,
          5,
          4,
          1,
          12,
          1,
          3,
          2,
          1,
          3,
          1,
          1,
          2,
          1,
          3,
          1,
          3,
          5,
          2,
          1,
          6,
          2,
          1,
          1,
          2,
          2,
          2,
          5,
          4,
          1,
          1,
          4,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          3,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          1,
          3,
          1,
          2,
          2,
          2,
          1,
          5,
          2,
          2,
          3,
          2,
          1,
          1,
          1,
          1,
          2,
          2,
          1,
          1,
          1,
          3,
          1,
          1,
          1,
          3,
          2,
          1,
          1,
          2,
          2,
          2,
          4,
          2,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          3,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          4,
          1,
          2,
          1,
          1,
          1,
          3,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          4,
          1,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          2,
          3,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Log log distribution of degrees"
        },
        "width": 1000,
        "xaxis": {
         "range": [
          -0.1,
          2.5
         ],
         "title": {
          "text": "Node degree"
         },
         "type": "log"
        },
        "yaxis": {
         "range": [
          -0.1,
          4
         ],
         "title": {
          "text": "Node count"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_degree_distribution(\n",
    "    degrees\n",
    "):\n",
    "\n",
    "    c= Counter(degrees)\n",
    "    x_val = list(c.keys())\n",
    "    y_val = [c[i] for i in x_val]\n",
    "    print('Maximum degree: {}'.format(int(max(x_val))))\n",
    "    print('Max node count with the same degree: {}'.format(int(max(y_val))))\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(mode='markers', x=x_val, y=y_val ))\n",
    "\n",
    "    fig.update_xaxes(type=\"log\", range=[-0.1,2.5]) # log range: 10^0=1, 10^5=100000\n",
    "    fig.update_yaxes(type=\"log\", range=[-0.1,4]) # linear range\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        title_text=\"Log log distribution of degrees\",\n",
    "        xaxis_title='Node degree',\n",
    "        yaxis_title='Node count')\n",
    "    return fig\n",
    "\n",
    "plot_degree_distribution(degrees_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arts ': {' Arts ': {' General': {'count': 5}, 'count': 3310, ' Architecture': {'count': 249}, ' Visual arts': {'count': 500}, ' Cultural venues': {'count': 131}, ' Literature': {'count': 989}, ' Music': {'count': 803}, ' Performing arts': {'count': 198}, ' Modern visual arts': {'count': 301}, ' Fictional characters': {'count': 134}}, 'count': 3310}, 'Philosophy and religion ': {' Philosophy and religion ': {' Abrahamic religions': {'count': 411}, 'count': 1408, ' Philosophy': {'count': 197}, ' Religion and spirituality': {'count': 170}, ' Eastern religions': {'count': 194}, ' Other religions': {'count': 71}, ' Mythology': {'count': 365}}, 'count': 1408}, 'Everyday life ': {' Sports, games and recreation ': {' Sports': {'count': 546}, 'count': 1231, ' Entertainment': {'count': 518}, ' Sports organizations': {'count': 167}}, 'count': 2422, ' Everyday life ': {' Cooking, food and drink': {'count': 654}, 'count': 1191, ' Clothing and fashion': {'count': 192}, ' General': {'count': 3}, ' Home living': {'count': 18}, ' Family and kinship': {'count': 97}, ' Household items': {'count': 74}, ' Sexuality and gender': {'count': 139}, ' Stages of life': {'count': 14}}}, 'Technology ': {' Technology ': {' Industry': {'count': 201}, 'count': 3148, ' General': {'count': 89}, ' Biotechnology': {'count': 48}, ' Computing and information technology': {'count': 741}, ' Energy': {'count': 172}, ' Navigation and timekeeping': {'count': 70}, ' Agriculture': {'count': 237}, ' Electronics': {'count': 117}, ' Infrastructure': {'count': 234}, ' Machinery and tools': {'count': 271}, ' Medical technology': {'count': 17}, ' Military technology': {'count': 303}, ' Optical technology': {'count': 57}, ' Astronomical technology': {'count': 72}, ' Space': {'count': 68}, ' Transportation': {'count': 451}}, 'count': 3148}, 'Biological and health sciences ': {' Animals ': {' General classifications': {'count': 13}, 'count': 2396, ' Animal breeds and hybrids': {'count': 149}, ' Individual animals': {'count': 94}, ' Mammals': {'count': 539}, ' Birds': {'count': 382}, ' Reptiles': {'count': 297}, ' Proto-mammals': {'count': 18}, ' Reptiliomorphs': {'count': 3}, ' Amphibians': {'count': 55}, ' Fishes': {'count': 299}, ' Agnatha': {'count': 17}, ' Crustaceans': {'count': 84}, ' Arachnids': {'count': 52}, ' Insects': {'count': 196}, ' Arthropoda, others': {'count': 35}, ' Mollusks': {'count': 69}, ' Cnidarians': {'count': 22}, ' Echinoderms': {'count': 15}, ' Porifera': {'count': 5}, ' Invertebrata, others': {'count': 52}}, 'count': 4681, ' Biology ': {' Biology basics': {'count': 47}, 'count': 886, ' Anatomy and morphology': {'count': 267}, ' Biological processes and physiology': {'count': 111}, ' Biochemistry and molecular biology': {'count': 160}, ' Zoology': {'count': 42}, ' Botany': {'count': 9}, ' Cell biology': {'count': 78}, ' Ecology': {'count': 39}, ' Genetics': {'count': 41}, ' Evolutionary biology': {'count': 92}}, ' Health ': {' Medicine': {'count': 199}, 'count': 791, ' Morbidity': {'count': 378}, ' Drugs and medication': {'count': 121}, ' Health and fitness': {'count': 93}}, ' Plants ': {' Plants': {'count': 503}, 'count': 608, ' Fungi': {'count': 36}, ' Other organisms': {'count': 69}}}, 'Physical sciences ': {' Chemistry ': {' Organic compounds': {'count': 194}, 'count': 1207, ' Basics': {'count': 136}, ' Chemical elements': {'count': 204}, ' Acids and bases': {'count': 54}, ' Alloys and ceramic compounds': {'count': 39}, ' Molecular compounds': {'count': 61}, ' Salts and ions': {'count': 220}, ' Analysis': {'count': 40}, ' Reactions and synthesis': {'count': 81}, ' Separation processes': {'count': 56}, ' Chemical engineering': {'count': 12}, ' Theory': {'count': 110}}, 'count': 4290, ' Astronomy ': {' Astronomy basics': {'count': 35}, 'count': 886, ' Physical cosmology': {'count': 51}, ' Observational astronomy': {'count': 49}, ' Astronomical objects': {'count': 302}, ' Celestial mechanics': {'count': 72}, ' Celestial sphere': {'count': 122}, ' Planetary science': {'count': 50}, ' Stellar astronomy': {'count': 116}, ' Galactic and extragalactic astronomy': {'count': 89}}, ' Earth science ': {' Air': {'count': 147}, 'count': 849, ' Earth': {'count': 629}, ' Earth science basics': {'count': 1}, ' Water': {'count': 69}, ' Biological materials': {'count': 3}}, ' Basics and measurement ': {' Science basics': {'count': 43}, 'count': 360, '  Science in countries': {'count': 15}, ' Measurement': {'count': 302}}, ' Physics ': {' Electromagnetism': {'count': 121}, 'count': 988, ' Condensed matter physics': {'count': 78}, ' Atomic, molecular and optical physics': {'count': 93}, ' Physics basics': {'count': 62}, ' Color': {'count': 53}, ' Waves': {'count': 48}, ' Mechanics': {'count': 273}, ' Nuclear physics': {'count': 25}, ' Particle physics': {'count': 111}, ' Theory of relativity': {'count': 35}, ' Thermodynamics': {'count': 89}}}, 'Geography ': {' Cities ': {' Urban studies and planning': {'count': 44}, 'count': 2030, ' Africa': {'count': 226}, ' Americas': {'count': 387}, ' Asia': {'count': 873}, ' Europe and Russia': {'count': 440}, ' Oceania': {'count': 43}, ' Non-city settlements': {'count': 17}}, 'count': 5318, ' Physical ': {' Islands': {'count': 490}, 'count': 1902, ' Peninsulas and capes': {'count': 86}, ' Land relief': {'count': 226}, ' Basics': {'count': 78}, ' Geography by country': {'count': 52}, ' Bodies of water': {'count': 529}, ' Other hydrological features': {'count': 109}, ' Isthmuses': {'count': 10}, ' Mountain peaks': {'count': 128}, ' Other terrestrial features': {'count': 89}, ' Parks, preserves, and World Heritage Sites': {'count': 105}}, ' Countries ': {' General': {'count': 19}, 'count': 1386, ' Countries': {'count': 221}, ' Regions and country subdivisions': {'count': 1146}}}, 'Society and social sciences ': {' Politic and economic ': {' Politics and government': {'count': 230}, 'count': 1825, ' Law': {'count': 567}, ' War and military': {'count': 110}, ' Business and economics': {'count': 668}, ' Organizations': {'count': 250}}, 'count': 4255, ' Culture ': {' Communication': {'count': 51}, 'count': 2075, ' Culture': {'count': 132}, ' Education': {'count': 353}, ' Ethnology and Anthropology': {'count': 120}, ' Language': {'count': 594}, ' Journalism and mass media': {'count': 825}}, ' Social studies ': {' Society': {'count': 140}, 'count': 355, ' Basics': {'count': 17}, ' Psychology': {'count': 138}, ' Sociology': {'count': 60}}}, 'History ': {' History ': {' Basics': {'count': 36}, 'count': 2979, ' History by continent and region': {'count': 42}, ' History by country and subdivision': {'count': 420}, ' History by city': {'count': 39}, ' History by ethnicity': {'count': 8}, ' Historical cities, towns and archaeological sites': {'count': 104}, ' History by topic': {'count': 323}, ' Prehistory': {'count': 23}, ' Ancient history': {'count': 300}, ' Post-classical history': {'count': 441}, ' Early modern history': {'count': 270}, ' 19th century': {'count': 238}, ' 20th century': {'count': 563}, ' 21st century': {'count': 172}}, 'count': 2979}, 'Mathematics ': {' Mathematics ': {' Basics': {'count': 114}, 'count': 1126, ' Functions': {'count': 88}, ' Geometry': {'count': 139}, ' Algebra': {'count': 74}, ' Statistics and probability': {'count': 85}, ' Foundations': {'count': 160}, ' Discrete mathematics': {'count': 140}, ' Calculus and analysis': {'count': 177}, ' Number theory': {'count': 50}, ' Applied mathematics': {'count': 36}, ' Theoretical computer science': {'count': 63}}, 'count': 1126}, 'People ': {' Artists, musicians, and composers ': {' Visual artists': {'count': 807}, 'count': 2310, ' Musicians and composers': {'count': 1503}}, 'count': 15575, ' Entertainers, directors, producers, and screenwriters ': {' Entertainers': {'count': 1424}, 'count': 2342, ' Dancers and choreographers': {'count': 200}, ' Comedians': {'count': 219}, ' Television hosts and personalities': {'count': 201}, ' Other entertainment and fields': {'count': 298}}, ' Military personnel, revolutionaries, and activists ': {' Military leaders, personnel, and theorists': {'count': 470}, 'count': 1012, ' Rebels, revolutionaries and activists': {'count': 542}}, ' Miscellaneous ': {' Businesspeople': {'count': 369}, 'count': 1186, ' Explorers': {'count': 275}, ' Law enforcement and fire service': {'count': 60}, ' Crime': {'count': 250}, ' Chefs, bartenders and winemakers': {'count': 40}, ' Hobbyists': {'count': 10}, ' Case studies': {'count': 25}, ' Cosmetics people': {'count': 10}, ' Sex work': {'count': 20}, ' Socialites': {'count': 40}, ' Fandom, conventions and festivals': {'count': 10}, ' Body modification': {'count': 10}, ' Micronations': {'count': 2}, ' Health and fitness': {'count': 6}, ' Tradespeople': {'count': 8}, ' Pseudoscientists': {'count': 40}, ' Other': {'count': 11}}, ' Philosophers, historians, political and social scientists ': {' Philosophers': {'count': 295}, 'count': 1335, ' Historians': {'count': 175}, ' Social scientists': {'count': 865}}, ' Politicians and leaders ': {' Ancient': {'count': 280}, 'count': 2452, ' Post-classical': {'count': 357}, ' Early modern period [about 1400-1814]': {'count': 383}, ' Modern [about 1815-1945]': {'count': 501}, ' Post-1945': {'count': 931}}, ' Religious figures ': {' Hinduism': {'count': 37}, 'count': 500, ' Buddhism': {'count': 60}, ' Judaism': {'count': 29}, ' Christianity': {'count': 241}, ' Islam': {'count': 90}, ' Others': {'count': 43}}, ' Scientists, inventors, and mathematicians ': {' Pre-modern figures': {'count': 58}, 'count': 1108, ' Physics and astronomy': {'count': 338}, ' Chemistry': {'count': 79}, ' Life sciences': {'count': 221}, ' Physical geography': {'count': 47}, ' Inventors and engineers': {'count': 185}, ' Mathematicians': {'count': 129}, ' Computer scientists and programmers': {'count': 51}}, ' Sports figures ': {' Association football': {'count': 110}, 'count': 1210, ' Basketball': {'count': 60}, ' Baseball': {'count': 56}, ' Cricket': {'count': 60}, ' Ice hockey': {'count': 45}, ' American football': {'count': 40}, ' Tennis': {'count': 51}, ' Golf': {'count': 35}, ' Athletics': {'count': 91}, ' Figure skating': {'count': 30}, ' Martial arts': {'count': 85}, ' Cycling': {'count': 20}, ' Gymnastics': {'count': 25}, ' Auto racing': {'count': 40}, ' Swimming': {'count': 20}, ' Rugby': {'count': 30}, ' Tabletop games': {'count': 45}, ' Other individual sports': {'count': 151}, ' Other team sports': {'count': 59}, ' Ancient sports': {'count': 5}, ' Sports business-people, coaches, and referees': {'count': 152}}, ' Writers and journalists ': {' Writers': {'count': 1725}, 'count': 2120, ' Journalists': {'count': 395}}}}\n"
     ]
    }
   ],
   "source": [
    "class2_file = './outputs_save/__classes2.txt'\n",
    "\n",
    "d_class = dict()\n",
    "with open(class2_file, 'r', encoding='utf8') as f:\n",
    "    for l in f:\n",
    "        try:\n",
    "            wikiid, cl2 = l.strip().split('\\t')\n",
    "            c0, c1, c2 =  cl2.split('->-') \n",
    "            if not c0 in d_class.keys():\n",
    "                d_class[c0] = {c1:{c2:{'count': 0}, 'count':0}, 'count':0}\n",
    "            elif not c1 in d_class[c0].keys():\n",
    "                d_class[c0][c1] = {c2:{'count': 0}, 'count':0}\n",
    "            elif not c2 in d_class[c0][c1].keys():\n",
    "                d_class[c0][c1][c2] = {'count': 0}\n",
    "        except:\n",
    "            break\n",
    "\n",
    "with open(class2_file, 'r', encoding='utf8') as f:\n",
    "    for l in f:\n",
    "        try:\n",
    "            wikiid, cl2 = l.strip().split('\\t')\n",
    "            c0, c1, c2 =  cl2.split('->-') \n",
    "            d_class[c0]['count'] += 1\n",
    "            d_class[c0][c1]['count'] += 1\n",
    "            d_class[c0][c1][c2]['count'] += 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "class_hierarchy = open('./outputs/__class_hierarchy.txt', 'w', encoding='utf8')\n",
    "\n",
    "for k0 in sorted(d_class.keys()):\n",
    "    if not k0 == 'count':\n",
    "        class_hierarchy.write(f'{k0} ({d_class[k0][\"count\"]} articles)\\n')\n",
    "        for k1 in sorted(d_class[k0].keys()):\n",
    "            if not k1 == 'count':\n",
    "                class_hierarchy.write(f'\\t\\t{k1} ({d_class[k0][k1][\"count\"]} articles)\\n')\n",
    "                for k2 in sorted(d_class[k0][k1].keys()):\n",
    "                    if not k2 == 'count':\n",
    "                        class_hierarchy.write(f'\\t\\t\\t\\t{k2} ({d_class[k0][k1][k2][\"count\"]} articles)\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_hierarchy.close()\n",
    "\n",
    "\n",
    "print(d_class)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a7894608ab89e0fa5e7f3054d382d15f72b8150e13c1deeee8c546b63809c86"
  },
  "kernelspec": {
   "display_name": "Python ('wikidump-XML-processing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
